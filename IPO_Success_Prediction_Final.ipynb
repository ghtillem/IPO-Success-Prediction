{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGmANq5D41uX2yMtGHVro3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_8PrQItCy-C"
      },
      "outputs": [],
      "source": [
        "#You will need to download the SEC Filings archive Zip Files\n",
        "#Below Cleans and Joinging the 3 main SEC Datasets on company filings\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "zip_dir = \"/content/drive/MyDrive/FSE IPO Data/Raw Zip Files/FSE570\"\n",
        "output_dir = \"/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "#output_file = os.path.join(output_dir, \"SEC_filings_cleaned_final10.csv\")\n",
        "\n",
        "# Clean start\n",
        "if os.path.exists(output_file):\n",
        "    os.remove(output_file)\n",
        "    print(f\"ğŸ—‘ï¸ Deleted existing {output_file} for a clean rebuild.\")\n",
        "\n",
        "# Get sorted list of zip files\n",
        "zip_files = sorted([f for f in os.listdir(zip_dir) if f.endswith(\".zip\")])\n",
        "first_file = True  # Controls header writing\n",
        "\n",
        "# Loop through each ZIP\n",
        "for zip_filename in zip_files:\n",
        "    zip_path = os.path.join(zip_dir, zip_filename)\n",
        "    print(f\"\\nğŸ”„ Processing: {zip_filename}\")\n",
        "\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            text_files = [f for f in zip_ref.namelist() if f.endswith('.txt')]\n",
        "            temp_dfs = {}\n",
        "\n",
        "            # Extract .txt files\n",
        "            for text_file in text_files:\n",
        "                with zip_ref.open(text_file) as file:\n",
        "                    try:\n",
        "                        content = file.read().decode('utf-8')\n",
        "                    except UnicodeDecodeError:\n",
        "                        content = file.read().decode('latin-1')\n",
        "\n",
        "                    lines = content.strip().split('\\n')\n",
        "                    if not lines:\n",
        "                        continue\n",
        "\n",
        "                    header = lines[0].split('\\t')\n",
        "                    if len(header) == 1:\n",
        "                        header = lines[0].split()\n",
        "\n",
        "                    data = []\n",
        "                    for line in lines[1:]:\n",
        "                        values = line.split('\\t')\n",
        "                        if len(values) == 1:\n",
        "                            values = line.split()\n",
        "                        if len(values) == len(header):\n",
        "                            data.append(dict(zip(header, values)))\n",
        "\n",
        "                    filename = os.path.splitext(os.path.basename(text_file))[0]\n",
        "                    temp_dfs[f\"{filename}_df\"] = pd.DataFrame(data)\n",
        "\n",
        "            # Grab required DataFrames\n",
        "            num_df = temp_dfs.get(\"num_df\")\n",
        "            tag_df = temp_dfs.get(\"tag_df\")\n",
        "            sub_df = temp_dfs.get(\"sub_df\")\n",
        "\n",
        "            if num_df is None or tag_df is None or sub_df is None:\n",
        "                print(f\"âš ï¸ Skipping {zip_filename}: missing num, tag, or sub.\")\n",
        "                continue\n",
        "\n",
        "            if \"custom\" not in tag_df.columns or \"abstract\" not in tag_df.columns:\n",
        "                print(f\"âš ï¸ Skipping {zip_filename}: 'custom' or 'abstract' column missing.\")\n",
        "                continue\n",
        "\n",
        "            # Filter tag table\n",
        "            tag_df[\"custom\"] = pd.to_numeric(tag_df[\"custom\"], errors=\"coerce\")\n",
        "            tag_df[\"abstract\"] = pd.to_numeric(tag_df[\"abstract\"], errors=\"coerce\")\n",
        "            tag_filtered_df = tag_df[(tag_df[\"custom\"] == 0) & (tag_df[\"abstract\"] == 0)]\n",
        "            print(f\"   âœ… tag_filtered_df: {len(tag_filtered_df)} rows\")\n",
        "\n",
        "            # Join num + tag\n",
        "            num_tag_df = pd.merge(num_df, tag_filtered_df, on=[\"tag\", \"version\"], how=\"inner\")\n",
        "            print(f\"   âœ… num_tag_df: {len(num_tag_df)} rows\")\n",
        "\n",
        "            # Join with sub\n",
        "            combined_df = pd.merge(num_tag_df, sub_df, on=\"adsh\", how=\"inner\")\n",
        "            print(f\"   âœ… combined_df: {len(combined_df)} rows\")\n",
        "\n",
        "            # Add quarter info\n",
        "            quarter = zip_filename.replace(\"-archive.zip\", \"\")\n",
        "            combined_df[\"filing_quarter\"] = quarter\n",
        "\n",
        "            if len(combined_df) == 0:\n",
        "                print(f\"   âš ï¸ No rows to write for {zip_filename}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Write to file\n",
        "            combined_df.to_csv(output_file, mode='a', header=first_file, index=False)\n",
        "            print(f\"   ğŸ’¾ Appended {len(combined_df)} rows to {output_file}\")\n",
        "            first_file = False\n",
        "\n",
        "            # Add sleep delay\n",
        "            print(\"â³ Waiting 15 seconds before next quarter...\")\n",
        "            time.sleep(15)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error processing {zip_filename}: {e}\")\n",
        "\n",
        "# Wrap up\n",
        "print(f\"\\nğŸ‰ All zip files processed.\\nğŸ“ Final data saved to:\\n{output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Summary info on new Filings data\n",
        "sample_df = pd.read_csv(output_file, nrows=100)\n",
        "\n",
        "\n",
        "print(\"\\nğŸ“‹ Column Names:\")\n",
        "print(sample_df.columns.tolist())\n",
        "\n",
        "print(\"\\nğŸ§ª Data Types:\")\n",
        "print(sample_df.dtypes)\n",
        "\n",
        "\n",
        "summary = sample_df.describe(include='all').T\n",
        "summary['example_value'] = sample_df.iloc[0]\n",
        "summary = summary[['example_value']]\n",
        "summary.reset_index(inplace=True)\n",
        "summary.columns = ['column_name', 'example_value']\n",
        "\n",
        "print(\"\\nğŸ” Sample Values from First Row:\")\n",
        "display(summary)"
      ],
      "metadata": {
        "id": "4sFfcBvXDDnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Second we Get Ticker Data from Yahoo Finance on Tickers price start, 6 month, and 3 years\n",
        " #YF is a free API but we found we were not able to sucessfuly get enough tickers for a proper test set so we purchased a Cheap API with EODHD, this code will be next cell block\n",
        "\n",
        "#Getting Ticker Price Data from Yahoo Finance\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "import random\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "\n",
        "# Step 1: Load the full ticker list from the joined file.\n",
        "\n",
        "input_path = \"/content/my_drive/MyDrive/FSE IPO Data/Filtered Data/Final/ticker_union_unique_joined.csv\"\n",
        "ticker_data = pd.read_csv(input_path, dtype={'ticker': str, 'cik': str})\n",
        "ticker_data['ticker'] = ticker_data['ticker'].str.strip()\n",
        "ticker_data = ticker_data.drop_duplicates(subset=[\"ticker\", \"cik\"])\n",
        "ticker_list = ticker_data.to_dict(orient=\"records\")\n",
        "print(f\"ğŸš€ Loaded {len(ticker_list):,} tickers for processing.\")\n",
        "\n",
        "# =============================================================================\n",
        "# Step 2: Helper Function - Get the Price Nearest to a Target Date\n",
        "# =============================================================================\n",
        "def get_price_nearest_to(df, target_date):\n",
        "    if df.empty or pd.isna(target_date):\n",
        "        return pd.NA\n",
        "    if not isinstance(df.index, pd.DatetimeIndex):\n",
        "        df.index = pd.to_datetime(df.index)\n",
        "    nearest_date = min(df.index, key=lambda d: abs(d - target_date))\n",
        "    price = df.loc[nearest_date]['Close']\n",
        "    if isinstance(price, pd.Series):\n",
        "        price = price.iloc[0]\n",
        "    return price\n",
        "\n",
        "# =============================================================================\n",
        "# Step 3: Main Function to Fetch Ticker Data with Adjusted Prices\n",
        "# =============================================================================\n",
        "def get_ticker_info(row, max_retries=3):\n",
        "    tkr = row['ticker']\n",
        "    cik = row.get('cik', None)\n",
        "    method_used = \"failed\"\n",
        "    data = None\n",
        "\n",
        "    # Define download methods (all using adjusted data)\n",
        "    methods = [\n",
        "        # Method 0: Try getting max available history.\n",
        "        lambda: yf.download(tkr, period=\"max\", auto_adjust=True, progress=False),\n",
        "        # Method 1: Explicitly request from a very early date (1900) to today.\n",
        "        lambda: yf.download(tkr, start=\"1900-01-01\", end=datetime.date.today().strftime(\"%Y-%m-%d\"), auto_adjust=True, progress=False),\n",
        "        # Method 2: As a fallback, try a 10-year period.\n",
        "        lambda: yf.download(tkr, period=\"10y\", auto_adjust=True, progress=False)\n",
        "    ]\n",
        "\n",
        "    for i, method in enumerate(methods):\n",
        "        retries = 0\n",
        "        while retries < max_retries:\n",
        "            try:\n",
        "                data_try = method()\n",
        "                if not data_try.empty:\n",
        "                    data = data_try\n",
        "                    # Label the method used for later reference.\n",
        "                    method_used = [\"max\", \"1900-now\", \"10y\"][i]\n",
        "                    break  # Exit the retry loop if data is successfully fetched.\n",
        "            except Exception as e:\n",
        "                print(f\"Method {i} for ticker {tkr} failed on retry {retries+1} with error: {e}\")\n",
        "            retries += 1\n",
        "            sleep_time = 2 ** retries  # Exponential backoff.\n",
        "            print(f\"Retrying ticker {tkr} in {sleep_time} seconds...\")\n",
        "            time.sleep(sleep_time)\n",
        "        if data is not None and not data.empty:\n",
        "            break  # Exit if we have successfully fetched data.\n",
        "\n",
        "    # Add a small random delay to be a good API citizen.\n",
        "    time.sleep(random.uniform(0.5, 1.5))\n",
        "\n",
        "    if data is not None and not data.empty:\n",
        "        if not isinstance(data.index, pd.DatetimeIndex):\n",
        "            data.index = pd.to_datetime(data.index)\n",
        "        start_date = data.index.min()\n",
        "        end_date = data.index.max()\n",
        "        six_months = start_date + pd.DateOffset(months=6)\n",
        "        three_years = start_date + pd.DateOffset(years=3)\n",
        "        price_start = get_price_nearest_to(data, start_date)\n",
        "        price_6mo = get_price_nearest_to(data, six_months)\n",
        "        price_3yr = get_price_nearest_to(data, three_years)\n",
        "        return {\n",
        "            \"ticker\": tkr,\n",
        "            \"cik\": cik,\n",
        "            \"start_date\": start_date,\n",
        "            \"end_date\": end_date,\n",
        "            \"method\": method_used,\n",
        "            \"date_6mo\": six_months,\n",
        "            \"date_3yr\": three_years,\n",
        "            \"price_start\": price_start,\n",
        "            \"price_6mo\": price_6mo,\n",
        "            \"price_3yr\": price_3yr\n",
        "        }\n",
        "    else:\n",
        "        print(f\"âŒ Failed to retrieve data for ticker {tkr} after {max_retries} retries per method.\")\n",
        "        return {\n",
        "            \"ticker\": tkr,\n",
        "            \"cik\": cik,\n",
        "            \"start_date\": pd.NaT,\n",
        "            \"end_date\": pd.NaT,\n",
        "            \"method\": method_used,\n",
        "            \"date_6mo\": pd.NaT,\n",
        "            \"date_3yr\": pd.NaT,\n",
        "            \"price_start\": pd.NA,\n",
        "            \"price_6mo\": pd.NA,\n",
        "            \"price_3yr\": pd.NA\n",
        "        }\n",
        "\n",
        "# =============================================================================\n",
        "# Step 4: Process All Tickers Concurrently\n",
        "# =============================================================================\n",
        "max_threads = 5  # Adjust as needed based on your system and API limits.\n",
        "results = []\n",
        "start_time = time.time()\n",
        "print(f\"âš™ï¸ Fetching data for {len(ticker_list):,} tickers using {max_threads} threads...\")\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=max_threads) as executor:\n",
        "    futures = [executor.submit(get_ticker_info, row) for row in ticker_list]\n",
        "    for i, future in enumerate(as_completed(futures)):\n",
        "        try:\n",
        "            res = future.result()\n",
        "            results.append(res)\n",
        "            if (i+1) % 100 == 0:\n",
        "                print(f\"Processed {i+1} tickers so far...\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error processing ticker at index {i}: {e}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"âœ… Full run complete in {end_time - start_time:.2f} seconds!\")\n",
        "\n",
        "# =============================================================================\n",
        "# Step 5: Create a DataFrame from the Results and Review Summary\n",
        "# =============================================================================\n",
        "df_results = pd.DataFrame(results)\n",
        "print(\"=== Final Ticker Information ===\")\n",
        "print(df_results.head())\n",
        "\n",
        "# Optional: Save the results to a CSV file if desired.\n",
        "# Uncomment the following lines to save the full results back to Google Drive:\n",
        "output_path = \"/content/my_drive/MyDrive/FSE IPO Data/Filtered Data/Final/full_ticker_data_with_adjusted_prices.csv\"\n",
        "df_results.to_csv(output_path, index=False)\n",
        "print(\"âœ… Output saved to:\", output_path)"
      ],
      "metadata": {
        "id": "hvNBVz5PFebO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Paid API Pull from EODHD financial repo\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "import logging\n",
        "\n",
        "TICKERS = tickers\n",
        "\n",
        " #[t + \".US\" for t in tickers[:5]]\n",
        "\n",
        "# EODHD API key\n",
        "API_KEY = \"API KEY\"\n",
        "\n",
        "# --- Logging setup ---\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def fetch_ticker_data(ticker, from_dt, to_dt, api_key):\n",
        "    url = (\n",
        "        f\"https://eodhd.com/api/eod/{ticker}\"\n",
        "        f\"?from={from_dt}&to={to_dt}\"\n",
        "        f\"&period=d&api_token={api_key}&fmt=json\"\n",
        "    )\n",
        "    try:\n",
        "        r = requests.get(url, timeout=10)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        if isinstance(data, dict) and data.get(\"warning\"):\n",
        "            logger.warning(f\"{ticker}: {data['warning']}\")\n",
        "            return None\n",
        "        if not data or 'date' not in data[0]:\n",
        "            logger.warning(f\"{ticker}: no usable data\")\n",
        "            return None\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        df.set_index('date', inplace=True)\n",
        "        return df[['adjusted_close']]\n",
        "    except Exception as e:\n",
        "        logger.error(f\"{ticker}: fetch failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def calculate_metrics(ticker, start_str, df):\n",
        "    # ensure sorted\n",
        "    df = df.sort_index()\n",
        "\n",
        "    # parse start date (must be in index)\n",
        "    start_dt = pd.to_datetime(start_str)\n",
        "\n",
        "    # exact first price\n",
        "    try:\n",
        "        sp = df.loc[start_dt, 'adjusted_close']\n",
        "    except KeyError:\n",
        "        # fallback if for some reason start_str missing\n",
        "        sp = df['adjusted_close'].iloc[0]\n",
        "\n",
        "    # sixâ€month and threeâ€year asof lookups\n",
        "    six_m_dt = start_dt + timedelta(days=180)\n",
        "    three_y_dt = start_dt + timedelta(days=365*3)\n",
        "\n",
        "    s6 = df['adjusted_close'].asof(six_m_dt)\n",
        "    s3 = df['adjusted_close'].asof(three_y_dt)\n",
        "\n",
        "    # 30-day window around six-month date\n",
        "    window = pd.date_range(six_m_dt - timedelta(days=15),\n",
        "                           six_m_dt + timedelta(days=15))\n",
        "    arr6 = df['adjusted_close'].loc[\n",
        "        df.index.to_series().between(window.min(), window.max())\n",
        "    ]\n",
        "    avg6 = arr6.mean() if not arr6.empty else (s6 if pd.notna(s6) else None)\n",
        "\n",
        "    # 31-day centered SMA and then average over 3-year period\n",
        "    df['sma_30'] = df['adjusted_close'].rolling(31, center=True).mean()\n",
        "    avg3 = df['sma_30'].loc[start_dt:three_y_dt].mean()\n",
        "\n",
        "    return {\n",
        "        'ticker':            ticker,\n",
        "        'start_date':        start_str,\n",
        "        'six_month_date':    six_m_dt.strftime('%Y-%m-%d'),\n",
        "        'three_year_date':   three_y_dt.strftime('%Y-%m-%d'),\n",
        "        'start_price':       round(sp, 2) if pd.notna(sp) else None,\n",
        "        'six_month_price':   round(s6, 2) if pd.notna(s6) else None,\n",
        "        'three_year_price':  round(s3, 2) if pd.notna(s3) else None,\n",
        "        'six_month_30d_avg': round(avg6, 2) if pd.notna(avg6) else None,\n",
        "        'three_year_30d_avg':round(avg3, 2) if pd.notna(avg3) else None,\n",
        "    }\n",
        "\n",
        "def test_batch_tickers():\n",
        "    today = datetime.today().strftime('%Y-%m-%d')\n",
        "\n",
        "    # prepare results DataFrame with all nine columns\n",
        "    results_df = pd.DataFrame(columns=[\n",
        "        'ticker','start_date','six_month_date','three_year_date',\n",
        "        'start_price','six_month_price','three_year_price',\n",
        "        'six_month_30d_avg','three_year_30d_avg'\n",
        "    ])\n",
        "\n",
        "    for ticker in TICKERS:\n",
        "        logger.info(f\"Fetching {ticker} â€¦\")\n",
        "        df = fetch_ticker_data(ticker, \"2000-01-01\", today, API_KEY)\n",
        "        if df is None or df.empty:\n",
        "            logger.warning(f\"Skipping {ticker}: no data\")\n",
        "            continue\n",
        "\n",
        "        # actual first trading date from the fetched data\n",
        "        actual_start = df.index.min()\n",
        "        start_str = actual_start.strftime('%Y-%m-%d')\n",
        "\n",
        "        # calculate and append\n",
        "        metrics = calculate_metrics(ticker, start_str, df)\n",
        "        results_df.loc[len(results_df)] = metrics\n",
        "\n",
        "    print(\"----- Final Results DataFrame -----\")\n",
        "    display(results_df)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_batch_tickers()\n",
        "\n",
        "file_path = '/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/ticker_metrics_output_EODHD.csv'\n",
        "\n"
      ],
      "metadata": {
        "id": "m_5tfbQWFkPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#third we get a mapping file created by joining two data sources from the SEC linking CIK (Centeral Index Key) to IPO Tickers. Look up SEC Company_tickers.josn and SEC ticker.txt\n",
        "\n",
        "json_path = \"/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/company_tickers.json\"\n",
        "with open(json_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Convert the JSON dictionary to a DataFrame.\n",
        "cik_map_df = pd.DataFrame.from_dict(data, orient='index')\n",
        "cik_map_df.columns = ['cik_str', 'ticker', 'title']\n",
        "\n",
        "# Create a properly formatted 'cik' column padded to 10 characters.\n",
        "cik_map_df['cik'] = cik_map_df['cik_str'].apply(lambda x: str(x).zfill(10))\n",
        "\n",
        "# Rearrange columns (optionally drop the original 'cik_str').\n",
        "cik_map_df = cik_map_df[['cik', 'ticker', 'title']]\n",
        "\n",
        "print(\"âœ… First few rows from the JSON file:\")\n",
        "print(cik_map_df.head())\n",
        "print(f\"Total records in JSON: {len(cik_map_df)}\\n\")\n",
        "\n",
        "# --------------------------\n",
        "# 2. Load and Process the Ticker Text File\n",
        "# --------------------------\n",
        "ticker_file_path = \"/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/ticker.txt\"\n",
        "ticker_df = pd.read_csv(ticker_file_path, sep='\\t', names=[\"ticker\", \"cik\"])\n",
        "\n",
        "# Ensure the 'cik' values are 10 characters long (padded with zeros if needed).\n",
        "ticker_df['cik'] = ticker_df['cik'].apply(lambda x: str(x).zfill(10))\n",
        "\n",
        "# Convert all ticker values to uppercase to match the JSON file's format.\n",
        "ticker_df['ticker'] = ticker_df['ticker'].str.upper()\n",
        "\n",
        "print(\"âœ… First few rows from the ticker text file (tickers capitalized):\")\n",
        "print(ticker_df.head())\n",
        "print(f\"Total records in ticker file: {len(ticker_df)}\\n\")\n",
        "\n",
        "# --------------------------\n",
        "# 3. Merge the DataFrames with an Outer Join\n",
        "# --------------------------\n",
        "merged_df = pd.merge(ticker_df, cik_map_df, on=['cik', 'ticker'], how='outer', indicator=True)\n",
        "\n",
        "print(\"âœ… Merged DataFrame preview:\")\n",
        "print(merged_df.head())\n",
        "print(\"\\nMerge source counts:\")\n",
        "print(merged_df['_merge'].value_counts())\n",
        "print(\"\\n\")\n",
        "\n",
        "# --------------------------\n",
        "# 4. Update the Merge Indicator Labels\n",
        "# --------------------------\n",
        "# Replace the merge indicator values:\n",
        "# - 'left_only' becomes 'ticker_txt'\n",
        "# - 'right_only' becomes 'comp_tick_json'\n",
        "# - 'both' stays unchanged\n",
        "merged_df['_merge'] = merged_df['_merge'].replace({\n",
        "    'left_only': 'ticker_txt',\n",
        "    'right_only': 'comp_tick_json'\n",
        "})\n",
        "\n",
        "# --------------------------\n",
        "# 5. Create the Union (ticker_union_unique)\n",
        "# --------------------------\n",
        "ticker_union_unique = merged_df.copy()\n",
        "\n",
        "# --------------------------\n",
        "# 6. Display the Union Results\n",
        "# --------------------------\n",
        "print(\"âœ… Ticker Union Unique DataFrame (all unique rows with origin labels):\")\n",
        "print(ticker_union_unique.head())\n",
        "print(\"\\nFull merge source counts after label updates:\")\n",
        "print(ticker_union_unique['_merge'].value_counts())\n",
        "\n",
        "# --------------------------\n",
        "# 7. Save the Union Results to a CSV File\n",
        "# --------------------------\n",
        "ticker_union_unique.to_csv(\"/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/ticker_union_unique.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "idMeHlevDPp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#finding number of unique companies (CIK) as some have multiple tickers\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "ipo_cik_file = \"/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/ticker_union_unique.csv\"\n",
        "output_cik_file = \"/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/cik_unique.csv\"\n",
        "\n",
        "# âœ… Load and extract unique CIKs\n",
        "ipo_df = pd.read_csv(ipo_cik_file)\n",
        "\n",
        "# Ensure CIK column exists and is normalized\n",
        "ipo_df['cik'] = ipo_df['cik'].astype(str).str.zfill(10)\n",
        "cik_unique_df = ipo_df[['cik']].drop_duplicates().sort_values('cik')\n",
        "\n",
        "# âœ… Save to file\n",
        "cik_unique_df.to_csv(output_cik_file, index=False)\n",
        "\n",
        "print(f\"âœ… Saved {len(cik_unique_df):,} unique CIKs to {output_cik_file}\")"
      ],
      "metadata": {
        "id": "fe82xlspDi1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we want to filter only IPOs which started in 2009 and forward as our SEC filings dataset only goes back this far\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "in_path = \"/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/ticker_metrics_output_EODHD.csv\"\n",
        "\n",
        "# read, parsing the start_date column as datetime\n",
        "df = pd.read_csv(in_path, parse_dates=[\"start_date\"])\n",
        "\n",
        "# filter for dates in 2009 or later\n",
        "df_2009_on = df[df[\"start_date\"] >= \"2009-01-01\"]\n",
        "\n",
        "# write out filtered file\n",
        "out_path = \"/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/ticker_metrics_2009on.csv\"\n",
        "df_2009_on.to_csv(out_path, index=False)\n",
        "\n",
        "print(f\"Filtered down from {len(df)} rows to {len(df_2009_on)} rows. Saved to:\\n{out_path}\")\n"
      ],
      "metadata": {
        "id": "4gzlRVb0D0Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now with our final filtered ticker file we add in the CIK\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "ipo_2009_df = pd.read_csv('/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/ticker_metrics_2009on.csv')\n",
        "ticker_union_df = pd.read_csv('/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/ticker_union_unique.csv')\n",
        "\n",
        "# Merge on 'ticker'\n",
        "merged_df = pd.merge(ipo_2009_df, ticker_union_df, on='ticker', how='inner')\n",
        "\n",
        "# Count distinct CIKs\n",
        "print(f\"âœ… After join, merged dataset has {len(merged_df)} rows and {merged_df['cik'].nunique()} distinct CIKs.\")\n",
        "\n",
        "# Save merged dataset\n",
        "merged_df.to_csv('/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/ticker_metrics_2009on_with_CIK.csv', index=False)"
      ],
      "metadata": {
        "id": "DzNs9aRhD3WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#If a company had more than one active ticker we chose the youngest one such that we can pull additional data on company filings\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load your merged dataset\n",
        "merged_df = pd.read_csv('/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/ticker_metrics_2009on_with_CIK.csv')\n",
        "\n",
        "# Make sure start_date is parsed as datetime\n",
        "merged_df['start_date'] = pd.to_datetime(merged_df['start_date'])\n",
        "\n",
        "# Sort by CIK and start_date (latest first)\n",
        "merged_sorted = merged_df.sort_values(['cik', 'start_date'], ascending=[True, False])\n",
        "\n",
        "# For each CIK, keep the first (latest start_date)\n",
        "cik_youngest_df = merged_sorted.drop_duplicates(subset='cik', keep='first')\n",
        "\n",
        "# Reset index\n",
        "cik_youngest_df = cik_youngest_df.reset_index(drop=True)\n",
        "\n",
        "# Report\n",
        "print(f\"âœ… After filtering for youngest ticker, {len(cik_youngest_df)} unique CIKs remain.\")\n",
        "\n",
        "# Save it\n",
        "cik_youngest_df.to_csv('/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/ticker_metrics_youngest_CIK_only.csv', index=False)\n"
      ],
      "metadata": {
        "id": "zPU_ZkDwEUxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adding fed data to Ticker data to get Macro economic variables\n",
        "\n",
        "# code below gets inflation rates and Interest rates from FRED\n",
        "\n",
        "#CPI (Consumer Price Index, All Urban Consumers, U.S. city average; CPIAUCSL):\n",
        "#https://fred.stlouisfed.org/graph/fredgraph.csv?id=CPIAUCSL\n",
        "\n",
        "#Federal Funds Rate (Effective; FEDFUNDS):\n",
        "#https://fred.stlouisfed.org/graph/fredgraph.csv?id=FEDFUNDS\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# â”€â”€ Step 1: load your cleaned IPO file â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "file_path = '/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/'\n",
        "infile   = file_path + '/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/ticker_metrics_youngest_CIK_only.csv' #'ticker_metrics_2009on_first_CIK_only.csv'\n",
        "df = pd.read_csv(infile, parse_dates=['start_date'])\n",
        "\n",
        "# â”€â”€ Step 2: load CPI & Fed Funds, using index_col=0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "cpi = pd.read_csv(\n",
        "    file_path + 'CPIAUCSL.csv',\n",
        "    index_col=0,\n",
        "    parse_dates=True,\n",
        "    comment='#'           # ignore any metadata lines\n",
        ")\n",
        "cpi.index.name = 'DATE'\n",
        "cpi.columns = ['CPI']    # rename column to a simple name\n",
        "\n",
        "fed = pd.read_csv(\n",
        "    file_path + 'FEDFUNDS.csv',\n",
        "    index_col=0,\n",
        "    parse_dates=True,\n",
        "    comment='#'\n",
        ")\n",
        "fed.index.name = 'DATE'\n",
        "fed.columns = ['FEDFUNDS']\n",
        "\n",
        "# â”€â”€ preprocess CPI â†’ YoY inflation % â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "cpi = cpi.sort_index()\n",
        "cpi['YoY_inflation'] = cpi['CPI'].pct_change(12) * 100\n",
        "\n",
        "# â”€â”€ preprocess Fed Funds â†’ forwardâ€fill daily rates â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "fed = fed.sort_index().ffill()\n",
        "\n",
        "# â”€â”€ Step 3: lookup function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def get_inflation_and_fed_rate(ipo_date):\n",
        "    # pick last CPI on or before the IPO\n",
        "    cpi_date = cpi.index[cpi.index <= ipo_date].max()\n",
        "    infl     = cpi.loc[cpi_date, 'YoY_inflation']\n",
        "\n",
        "    # pick last Fed Funds rate on or before the IPO\n",
        "    fed_date = fed.index[fed.index <= ipo_date].max()\n",
        "    fedr     = fed.loc[fed_date, 'FEDFUNDS']\n",
        "\n",
        "    return round(infl, 2), round(fedr, 2)\n",
        "\n",
        "# â”€â”€ Step 4: apply to your DataFrame â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "df[['inflation_rate_at_start', 'fed_funds_rate_at_start']] = \\\n",
        "    df['start_date'].apply(lambda dt: pd.Series(get_inflation_and_fed_rate(dt)))\n",
        "\n",
        "# â”€â”€ Step 5: save back to Drive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "outfile = file_path + 'EODHD_full_ticker_data_with_adjusted_prices_with_macro.csv'\n",
        "df.to_csv(outfile, index=False)\n",
        "print(f\"âœ… Done! saved {len(df)} rows â†’ {outfile}\")\n"
      ],
      "metadata": {
        "id": "_8Lp_fdYEWbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adding SnP500 Macro Data to Ticker file\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final\")\n",
        "\n",
        "# â”€â”€ 1. Load IPO file (and be 100 % sure start_date is datetime) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "ipo = pd.read_csv(BASE / \"EODHD_full_ticker_data_with_adjusted_prices_with_macro.csv\",\n",
        "                  parse_dates=[\"start_date\"])\n",
        "ipo[\"start_date\"] = pd.to_datetime(ipo[\"start_date\"], errors=\"coerce\")\n",
        "\n",
        "# â”€â”€ 2. A *robust* loader for the S&P-500 CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def load_sp500(p: Path) -> pd.DataFrame:\n",
        "    # Read the CSV, skipping the first 2 rows (metadata: \"Ticker,^GSPC\" and \"Date,NaN\")\n",
        "    df = pd.read_csv(p, skiprows=2, names=['Date', 'SP500_Close'])\n",
        "\n",
        "    # Print raw data for debugging\n",
        "    print(\"\\n--- Raw S&P 500 Data (First 10 rows) ---\")\n",
        "    print(df.head(10))\n",
        "\n",
        "    # Convert Date to datetime, specify format to avoid warnings\n",
        "    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d', errors='coerce')\n",
        "\n",
        "    # Drop rows where Date is NaT (invalid dates, e.g., \"Date\")\n",
        "    invalid_rows = df['Date'].isna()\n",
        "    if invalid_rows.any():\n",
        "        print(\"\\n--- Invalid S&P 500 Date Rows ---\")\n",
        "        print(df[invalid_rows][['Date', 'SP500_Close']])\n",
        "        df = df[~invalid_rows]\n",
        "\n",
        "    # Set Date as index\n",
        "    df.set_index('Date', inplace=True)\n",
        "\n",
        "    # Ensure SP500_Close is numeric\n",
        "    df['SP500_Close'] = pd.to_numeric(df['SP500_Close'], errors='coerce')\n",
        "\n",
        "    # Drop any rows with NaN in SP500_Close\n",
        "    df = df.dropna(subset=['SP500_Close'])\n",
        "\n",
        "    # Calculate moving averages\n",
        "    df['SMA_30_centered'] = df['SP500_Close'].rolling(window=31, center=True).mean()\n",
        "    df['SMA_50'] = df['SP500_Close'].rolling(window=50, min_periods=1).mean()\n",
        "    df['SMA_200'] = df['SP500_Close'].rolling(window=200, min_periods=1).mean()\n",
        "\n",
        "    # Round all numeric columns to 2 decimal places\n",
        "    numeric_columns = ['SP500_Close', 'SMA_30_centered', 'SMA_50', 'SMA_200']\n",
        "    df[numeric_columns] = df[numeric_columns].round(2)\n",
        "\n",
        "    # Sort index to ensure chronological order\n",
        "    df = df.sort_index()\n",
        "\n",
        "    # Validate data\n",
        "    print(\"\\nâœ… S&P 500 Data Loaded.\")\n",
        "    print(\"\\n--- Columns ---\")\n",
        "    print(df.columns.tolist())\n",
        "    print(\"\\n--- First 5 rows ---\")\n",
        "    print(df.head())\n",
        "    print(\"\\n--- Last 5 rows ---\")\n",
        "    print(df.tail())\n",
        "    print(\"\\n--- Date range ---\")\n",
        "    print(f\"Min: {df.index.min()}, Max: {df.index.max()}\")\n",
        "    print(\"\\n--- NaN counts ---\")\n",
        "    print(df.isna().sum())\n",
        "\n",
        "    return df\n",
        "\n",
        "sp500 = load_sp500(BASE / \"sp500_full_yahoo.csv\")\n",
        "\n",
        "# â”€â”€ 3. Helper to grab the S&P snapshot on IPO day â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def sp500_on(date: pd.Timestamp) -> pd.Series:\n",
        "    # Ensure date is a Timestamp\n",
        "    if not isinstance(date, pd.Timestamp):\n",
        "        return pd.Series({\n",
        "            \"sp500_close_at_start\": None,\n",
        "            \"sp500_sma30_centered_at_start\": None,\n",
        "            \"sp500_sma50_at_start\": None,\n",
        "            \"sp500_sma200_at_start\": None,\n",
        "            \"sp500_above_30sma_centered\": None,\n",
        "            \"sp500_above_50sma\": None,\n",
        "            \"sp500_above_200sma\": None,\n",
        "        })\n",
        "\n",
        "    # Find the closest date in S&P 500 data\n",
        "    loc = sp500.index.searchsorted(date, side=\"right\") - 1\n",
        "    if loc < 0 or loc >= len(sp500):\n",
        "        # IPO date is outside S&P 500 data range\n",
        "        print(f\"Warning: Date {date} is outside S&P 500 data range ({sp500.index.min()} to {sp500.index.max()})\")\n",
        "        return pd.Series({\n",
        "            \"sp500_close_at_start\": None,\n",
        "            \"sp500_sma30_centered_at_start\": None,\n",
        "            \"sp500_sma50_at_start\": None,\n",
        "            \"sp500_sma200_at_start\": None,\n",
        "            \"sp500_above_30sma_centered\": None,\n",
        "            \"sp500_above_50sma\": None,\n",
        "            \"sp500_above_200sma\": None,\n",
        "        })\n",
        "\n",
        "    row = sp500.iloc[loc]\n",
        "    return pd.Series({\n",
        "        \"sp500_close_at_start\": row.SP500_Close,\n",
        "        \"sp500_sma30_centered_at_start\": row.SMA_30_centered,\n",
        "        \"sp500_sma50_at_start\": row.SMA_50,\n",
        "        \"sp500_sma200_at_start\": row.SMA_200,\n",
        "        \"sp500_above_30sma_centered\": row.SP500_Close > row.SMA_30_centered if pd.notna(row.SMA_30_centered) else None,\n",
        "        \"sp500_above_50sma\": row.SP500_Close > row.SMA_50 if pd.notna(row.SMA_50) else None,\n",
        "        \"sp500_above_200sma\": row.SP500_Close > row.SMA_200 if pd.notna(row.SMA_200) else None,\n",
        "    })\n",
        "\n",
        "# â”€â”€ 4. Enrich IPO DataFrame and show the very first row â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "ipo = pd.concat([ipo, ipo[\"start_date\"].apply(sp500_on)], axis=1)\n",
        "\n",
        "print(\"\\n--- Quick preview of first row ---\")\n",
        "print(ipo.iloc[0])\n",
        "\n",
        "# â”€â”€ 5. If everything looks good, overwrite the file â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "ipo.to_csv(BASE / \"EODHD_full_ticker_data_with_adjusted_prices_with_macro_snp.csv\", index=False)"
      ],
      "metadata": {
        "id": "HuGpoZCvEaKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using OpenCorperates\n",
        "\n",
        "# â€” Imports â€”\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import re\n",
        "from urllib.parse import quote\n",
        "from google.colab import drive\n",
        "\n",
        "# â€” 1) Mount Drive and Load â€”\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/EODHD_full_ticker_data_with_adjusted_prices_with_macro_snp.csv' #EODHD_adjusted_macrosnp_joined_with_metadata_wsf.csv\n",
        "df = pd.read_csv(path)\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Pull ALL company names\n",
        "company_names = df['company_name'].astype(str).dropna().tolist()\n",
        "\n",
        "# â€” 2) Cleaning Helper â€”\n",
        "def clean_name(name):\n",
        "    name = re.sub(r'\\s*/.*$', '', name)   # remove \"/STATE/\" stuff\n",
        "    name = re.sub(\n",
        "        r',?\\s+(Inc\\.?|Corporation|Corp\\.?|LLC|L\\.L\\.C\\.|Co|Company|Ltd\\.?|Trust|Group|Holdings)$',\n",
        "        '',\n",
        "        name,\n",
        "        flags=re.IGNORECASE\n",
        "    ).strip()\n",
        "    return name\n",
        "\n",
        "# â€” 3) OpenCorporates Lookup â€”\n",
        "API_TOKEN = 'API Key'  # <<-- replace this\n",
        "results = []\n",
        "\n",
        "for idx, raw in enumerate(company_names):\n",
        "    query = clean_name(raw)\n",
        "    url   = (\n",
        "        f'https://api.opencorporates.com/v0.4/companies/search'\n",
        "        f'?q={quote(query)}'\n",
        "        f'&api_token={API_TOKEN}'\n",
        "    )\n",
        "    try:\n",
        "        r = requests.get(url)\n",
        "        r.raise_for_status()\n",
        "        comps = r.json()['results']['companies']\n",
        "        incorp = comps[0]['company'].get('incorporation_date') if comps else None\n",
        "    except Exception as e:\n",
        "        print(f\"Error for '{raw}': {e}\")\n",
        "        incorp = None\n",
        "\n",
        "    results.append({\n",
        "        'raw_name': raw,\n",
        "        'search_name': query,\n",
        "        'incorporation_date': incorp\n",
        "    })\n",
        "\n",
        "    if idx % 10 == 0:\n",
        "        print(f\"Processed {idx}/{len(company_names)} companies...\")\n",
        "\n",
        "    time.sleep(0.5)  # <= 2 requests per second\n",
        "\n",
        "# â€” 4) Save Output â€”\n",
        "out_df = pd.DataFrame(results)\n",
        "save_path = '/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/IPO_companies_with_incorp_dates.csv'\n",
        "out_df.to_csv(save_path, index=False)\n",
        "print(\"âœ… Done! Saved to:\", save_path)\n"
      ],
      "metadata": {
        "id": "O8XCP-IhHCzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#See list of compnay names with Incorpdates\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "default_path = '/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/IPO_companies_with_incorp_dates.csv'\n",
        "df = pd.read_csv(default_path)\n",
        "\n",
        "# Display the first few rows\n",
        "display(df.head())\n",
        "\n",
        "# Count nulls per column\n",
        "null_counts = df.isna().sum()\n",
        "print(\"Null counts per column:\")\n",
        "print(null_counts)\n",
        "\n",
        "# Total number of null values in the DataFrame\n",
        "total_nulls = null_counts.sum()\n",
        "print(f\"\\nTotal null values in the DataFrame: {total_nulls}\")\n",
        "\n",
        "# Optional: give a summary of non-null counts and dtypes\n",
        "df.info()"
      ],
      "metadata": {
        "id": "qFH0_BTyC5jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#joining the incorp dates into the EODHD ticker file\n",
        "import pandas as pd\n",
        "\n",
        "# Paths to your CSV files\n",
        "incorp_path = '/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/IPO_companies_with_incorp_dates.csv'\n",
        "meta_path   = '/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/EODHD_adjusted_macrosnp_joined_with_metadata_wsf.csv'\n",
        "\n",
        "# Load the datasets\n",
        "df_incorp = pd.read_csv(incorp_path)\n",
        "df_meta   = pd.read_csv(meta_path)\n",
        "\n",
        "# Perform the merge on raw_name and company_name (left join to keep all companies)\n",
        "merged = pd.merge(\n",
        "    df_incorp,\n",
        "    df_meta,\n",
        "    how='left',\n",
        "    left_on='raw_name',\n",
        "    right_on='company_name',\n",
        "    suffixes=('_incorp','_meta')\n",
        ")\n",
        "\n",
        "# Inspect the result\n",
        "print(\"Merged DataFrame head:\")\n",
        "display(merged.head())\n",
        "\n",
        "# Count nulls in the merged frame\n",
        "total_nulls = merged.isna().sum().sum()\n",
        "print(f\"\\nTotal null values after merge: {total_nulls}\")\n",
        "print(\"Null counts per column:\")\n",
        "print(merged.isna().sum())\n",
        "\n",
        "# Save the merged DataFrame to a new CSV\n",
        "out_path = '/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/EODHD_adjusted_macrosnp_joined_with_metadata_incorp_wsf.csv'\n",
        "merged.to_csv(out_path, index=False)\n",
        "print(f\"\\nSaved merged dataset to: {out_path}\")\n",
        "\n",
        "# Optional: summary of dtypes and non-null counts\n",
        "merged.info()"
      ],
      "metadata": {
        "id": "HRUC11nBCe73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "codKs_xOCWb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# â”€â”€ PATHS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "INPUT_FILE   = '/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/SEC_filings_cleaned_final10.csv'\n",
        "IPO_FILE     = '/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/ticker_metrics_youngest_CIK_only.csv'\n",
        "OUTPUT_FILE  = '/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/SEC_filings_filtered_before_youngest_CIK_RETAINED.csv'\n",
        "\n",
        "# â”€â”€ 1. BACK-UP ANY OLD OUTPUT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "out = Path(OUTPUT_FILE)\n",
        "if out.exists():\n",
        "    backup = out.with_suffix('.BACKUP_before_restart.csv')\n",
        "    shutil.copy(out, backup)\n",
        "    print(f'ğŸ”’ Existing output backed up to:\\n   {backup}')\n",
        "\n",
        "# â”€â”€ 2. LOAD IPO START DATES INTO A DICT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "ipo_df = (\n",
        "    pd.read_csv(IPO_FILE, usecols=['cik', 'start_date'])\n",
        "      .assign(start_date=lambda d: pd.to_datetime(d['start_date']))\n",
        ")\n",
        "cik_start = dict(zip(ipo_df['cik'].astype(str), ipo_df['start_date']))\n",
        "print(f'ğŸ“¥ Loaded {len(cik_start):,} CIK â†’ start_date mappings.')\n",
        "\n",
        "# â”€â”€ 3. STREAM THE SEC FILE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "CHUNK_SIZE = 2_500_000\n",
        "first_write = True\n",
        "\n",
        "for i, raw in enumerate(pd.read_csv(INPUT_FILE, chunksize=CHUNK_SIZE)):\n",
        "    print(f'ğŸ“¦ Chunk {i:,} | rows = {len(raw):,}')\n",
        "\n",
        "    # 3a) FIX & PARSE ddate\n",
        "    d = raw['ddate'].astype(str).str.replace(r'^3', '2', regex=True)\n",
        "    raw['ddate'] = pd.to_datetime(d, format='%Y%m%d', errors='coerce')\n",
        "    raw = raw.dropna(subset=['ddate'])\n",
        "\n",
        "    # 3b) KEEP ONLY OUR CIKs\n",
        "    raw['cik'] = raw['cik'].astype(str)\n",
        "    raw = raw[raw['cik'].isin(cik_start)]\n",
        "\n",
        "    # 3c) KEEP ONLY rows with ddate â‰¤ IPO start_date\n",
        "    keep_mask = raw.apply(lambda r: r['ddate'] <= cik_start[r['cik']], axis=1)\n",
        "    filt = raw[keep_mask]\n",
        "    print(f'  â†’ after filters: {len(filt):,} rows')\n",
        "\n",
        "    # 3d) WRITE / APPEND\n",
        "    if len(filt):\n",
        "        mode   = 'w' if first_write else 'a'\n",
        "        header = first_write\n",
        "        filt.to_csv(OUTPUT_FILE, index=False, mode=mode, header=header)\n",
        "        first_write = False\n",
        "\n",
        "print('\\nâœ… Finished. Filtered â€œpre-IPOâ€ data written to:')\n",
        "print(f'   {OUTPUT_FILE}')\n"
      ],
      "metadata": {
        "id": "6BO5DJAoHCui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6VOTXYNeJysY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#all in one Pivots data and builds filing features\n",
        "# ================================================================\n",
        "#      ROC + VALUE  FEATURES  â€¢  QUARTER-AWARE  (ALL CIKs)\n",
        "# ================================================================\n",
        "!pip install tqdm --quiet\n",
        "\n",
        "import pandas as pd, numpy as np, gc, sys\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "def pf(*a, **k): print(*a, **k, flush=True)\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# CONFIG\n",
        "# ----------------------------------------------------------------\n",
        "DATA_PATH = '/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/SEC_filings_filtered_before_youngest_CIK_RETAINED.csv'\n",
        "OUT_PATH  = '/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/EODHD_roc_value_stats_ALL_youngest_CIK_flat_Qcols.csv'\n",
        "\n",
        "ID_VARS   = ['cik', 'ddate', 'qtrs']\n",
        "WINDOWS   = {'All': None, '3Y': 3*365, '1Y': 365}   # in days\n",
        "\n",
        "META_DROP = ['adsh','period','fy','fp','filed','accepted',\n",
        "             'prevrpt','detail','nciks','aciks','filing_quarter']\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 1 â€¢ LOAD + CLEAN\n",
        "# ----------------------------------------------------------------\n",
        "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
        "pf(\"STEP 1 â€¢ raw shape\", df.shape)\n",
        "\n",
        "df['ddate'] = pd.to_datetime(df['ddate'], errors='coerce')\n",
        "df = df.dropna(subset=['ddate'])\n",
        "df.drop(columns=[c for c in META_DROP if c in df.columns], inplace=True)\n",
        "\n",
        "# â¬‡ï¸  capture every CIK *before* any filtering removes it\n",
        "all_ciks = df['cik'].unique().tolist()          #  <-- NEW LINE\n",
        "pf(\"STEP 1 â€¢ after cleaning\", df.shape)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 2 â€¢ MELT (numeric tag values)\n",
        "# ----------------------------------------------------------------\n",
        "tag_cols = [c for c in df.columns if c not in ID_VARS]\n",
        "df[tag_cols] = (df[tag_cols].apply(pd.to_numeric, errors='coerce')\n",
        "                              .replace(0, np.nan))\n",
        "\n",
        "df_long = (df.melt(id_vars=ID_VARS,\n",
        "                   value_vars=tag_cols,\n",
        "                   var_name='tag',\n",
        "                   value_name='value')\n",
        "             .dropna(subset=['value']))\n",
        "pf(\"STEP 2 â€¢ long shape\", df_long.shape)\n",
        "del df; gc.collect()\n",
        "\n",
        "# add helper for rolling windows\n",
        "df_long['days_from_latest'] = (\n",
        "    df_long.groupby('cik')['ddate'].transform('max') - df_long['ddate']\n",
        ").dt.days\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 3 â€¢ DAILY-ROC (mean)  by (cik, qtrs, tag)\n",
        "# ----------------------------------------------------------------\n",
        "def roc_stats(grp):\n",
        "    if len(grp) < 2:\n",
        "        return pd.Series({f'{w}_ROC': np.nan for w in WINDOWS})\n",
        "    grp = grp.sort_values('ddate')\n",
        "    grp['Î”d'] = (grp['ddate'] - grp['ddate'].shift()).dt.days\n",
        "    grp['Î”v'] = grp['value'].diff()\n",
        "    grp = grp[(grp['Î”d'] > 0) & grp['Î”v'].notna()]\n",
        "    if grp.empty:\n",
        "        return pd.Series({f'{w}_ROC': np.nan for w in WINDOWS})\n",
        "    roc = (grp['Î”v'] / grp['Î”d']).replace([np.inf,-np.inf], np.nan)\n",
        "    latest = grp['ddate'].max()\n",
        "    out = {}\n",
        "    for w,days in WINDOWS.items():\n",
        "        sel = roc if days is None else roc[(latest - grp['ddate']).dt.days <= days]\n",
        "        out[f'{w}_ROC'] = sel.mean()\n",
        "    return pd.Series(out)\n",
        "\n",
        "tqdm.pandas()\n",
        "roc = (df_long.groupby(['cik','qtrs','tag'], group_keys=False)\n",
        "             .progress_apply(roc_stats)\n",
        "             .reset_index())\n",
        "pf(\"STEP 3 â€¢ ROC table\", roc.shape)\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 4 â€¢ VALUE MEAN / MEDIAN  by (cik, qtrs, tag)\n",
        "# ----------------------------------------------------------------\n",
        "def val_stats(grp):\n",
        "    out = {}\n",
        "    for w,days in WINDOWS.items():\n",
        "        sel = grp['value'] if days is None else grp.loc[grp['days_from_latest'] <= days,'value']\n",
        "        out[f'{w}_mean']   = sel.mean()\n",
        "        out[f'{w}_median'] = sel.median()\n",
        "    return pd.Series(out)\n",
        "\n",
        "vals = (df_long.groupby(['cik','qtrs','tag'], group_keys=False)\n",
        "               .progress_apply(val_stats)\n",
        "               .reset_index())\n",
        "pf(\"STEP 4 â€¢ value table\", vals.shape)\n",
        "del df_long; gc.collect()\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 5 â€¢ PIVOT  â‡’  wide tables with Q-prefix\n",
        "# ----------------------------------------------------------------\n",
        "def pivot_wide(src, value_cols, suffix):\n",
        "    tables = []\n",
        "    for col in value_cols:\n",
        "        w = src.pivot(index='cik', columns=['qtrs','tag'], values=col)\n",
        "        # flatten multi-index columns\n",
        "        w.columns = [f\"Q{int(q)}_{t}_{suffix}\" for q,t in w.columns]\n",
        "        w.reset_index(inplace=True)\n",
        "        tables.append(w)\n",
        "    return tables\n",
        "\n",
        "roc_tables  = pivot_wide(roc,  [c for c in roc.columns if c.endswith('_ROC')], 'ROC')\n",
        "val_tables  = []\n",
        "for stat in vals.columns[3:]:        # skip cik,qtrs,tag\n",
        "    base = '_'.join(stat.split('_')[1:])      # mean / median\n",
        "    val_tables += pivot_wide(vals, [stat], base)\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 6 â€¢ MERGE ALL  (retain every CIK)\n",
        "# ----------------------------------------------------------------\n",
        "master = pd.DataFrame({'cik': all_ciks})        #  <-- CHANGED\n",
        "from functools import reduce\n",
        "for tbl in roc_tables + val_tables:\n",
        "    master = master.merge(tbl, on='cik', how='left')\n",
        "pf(\"STEP 6 â€¢ merged shape\", master.shape)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 7 â€¢ DROP all-NaN / all-zero / constant\n",
        "# ----------------------------------------------------------------\n",
        "drop = []\n",
        "for col in master.columns[1:]:\n",
        "    s = master[col].dropna()\n",
        "    if s.empty or (s==0).all() or (s.nunique()==1 and master.shape[0]>1):\n",
        "        drop.append(col)\n",
        "master.drop(columns=drop, inplace=True)\n",
        "pf(f\"STEP 7 â€¢ dropped {len(drop)} useless cols â†’ {master.shape}\")\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 8 â€¢ SAVE\n",
        "# ----------------------------------------------------------------\n",
        "master.to_csv(OUT_PATH, index=False)\n",
        "pf(f\"âœ…  Saved â†’ {OUT_PATH}  ({master.shape[0]} rows Ã— {master.shape[1]} cols)\")\n",
        "# files.download(OUT_PATH)  # uncomment if small enough\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aVx7GzJmJyik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "#      QUARTER-AWARE   2-YEAR  ROC + VALUE   (chunked)\n",
        "# ===============================================================\n",
        "!pip install tqdm --quiet\n",
        "import pandas as pd, numpy as np, gc\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "IN   = '/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/SEC_filings_filtered_before_youngest_IPO_EODHD.csv'\n",
        "OUT  = '/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/EODHD_2Y_roc_value_stats_flat.csv'\n",
        "\n",
        "CHUNK_ROWS = 8_000_000                        # ~45 GB peak on Colab Pro+\n",
        "WINDOW_DAYS = 2 * 365                         # â† exactly one window\n",
        "ID_VARS   = ['cik', 'qtrs', 'tag']\n",
        "DTYPES    = {'cik':'int32', 'qtrs':'int8', 'value':'float32'}\n",
        "\n",
        "# â”€â”€ PASS-1 : latest filing date per CIK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "latest = {}\n",
        "for chunk in tqdm(pd.read_csv(IN, usecols=['cik','ddate'],\n",
        "                              dtype={'cik':'int32','ddate':'string'},\n",
        "                              chunksize=CHUNK_ROWS), desc='latest-pass'):\n",
        "    chunk['ddate'] = pd.to_datetime(chunk['ddate'], errors='coerce')\n",
        "    for r in chunk.groupby('cik')['ddate'].max().items():\n",
        "        cik, dt = r\n",
        "        latest[cik] = max(latest.get(cik, pd.Timestamp('1900-01-01')), dt)\n",
        "del chunk; gc.collect()\n",
        "print(\"Latest dates collected for\", len(latest), \"CIKs\")\n",
        "\n",
        "# â”€â”€ accumulators â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "roc_sum = defaultdict(float);  roc_cnt = defaultdict(int)\n",
        "val_sum = defaultdict(lambda: {'mean':0.0,'median':0.0});  val_cnt = defaultdict(int)\n",
        "\n",
        "# â”€â”€ PASS-2 : chunk processing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "use_cols = ID_VARS + ['ddate','value']\n",
        "for chunk in tqdm(pd.read_csv(IN, usecols=use_cols,\n",
        "                              chunksize=CHUNK_ROWS, dtype=DTYPES),\n",
        "                  desc='stats-pass'):\n",
        "    chunk['ddate'] = pd.to_datetime(chunk['ddate'], errors='coerce')\n",
        "    chunk = chunk.dropna(subset=['value','ddate','tag'])\n",
        "    chunk['tag'] = chunk['tag'].astype('category')\n",
        "\n",
        "    for (cik,q,tag), grp in chunk.groupby(ID_VARS):\n",
        "        grp = grp.sort_values('ddate')\n",
        "        dd  = grp['ddate'].diff().dt.days\n",
        "        dv  = grp['value'].diff()\n",
        "        ok  = (dd>0) & dv.notna()\n",
        "        roc = (dv/dd)[ok].replace([np.inf,-np.inf], np.nan).dropna()\n",
        "\n",
        "        win_start = latest[cik] - pd.Timedelta(days=WINDOW_DAYS)\n",
        "        mask_win  = grp['ddate'][ok] >= win_start\n",
        "\n",
        "        # ----- ROC mean over 2-year window ----------------------\n",
        "        if len(roc[mask_win]):\n",
        "            key = (cik,q,tag,'ROC')\n",
        "            roc_sum[key] += roc[mask_win].sum()\n",
        "            roc_cnt[key] += roc[mask_win].count()\n",
        "\n",
        "        # ----- value mean + median over 2-year window ----------\n",
        "        sel = grp.loc[grp['ddate'] >= win_start, 'value']\n",
        "        if len(sel):\n",
        "            key = (cik,q,tag)\n",
        "            val_sum[key]['mean']   += sel.mean()\n",
        "            val_sum[key]['median'] += sel.median()\n",
        "            val_cnt[key]           += 1\n",
        "    del chunk, grp; gc.collect()\n",
        "\n",
        "# â”€â”€ build wide rows (one per CIK) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "rows = defaultdict(dict)\n",
        "\n",
        "# --- ROC ---\n",
        "for key, s in roc_sum.items():           # key = (cik, q, tag, 'ROC')\n",
        "    cik, q, tag, _ = key\n",
        "    cnt = roc_cnt[key]\n",
        "    rows[cik][f\"Q{q}_{tag}_2Y_ROC\"] = s / cnt\n",
        "\n",
        "# --- mean & median ---\n",
        "for key, s in val_sum.items():           # key = (cik, q, tag)\n",
        "    cik, q, tag = key\n",
        "    if val_cnt[key]:\n",
        "        rows[cik][f\"Q{q}_{tag}_2Y_mean\"]   = s['mean']   / val_cnt[key]\n",
        "        rows[cik][f\"Q{q}_{tag}_2Y_median\"] = s['median'] / val_cnt[key]\n",
        "\n",
        "\n",
        "wide = pd.DataFrame.from_dict(rows, orient='index').reset_index(names='cik')\n",
        "print(\"Raw wide shape:\", wide.shape)\n",
        "\n",
        "# â”€â”€ drop all-NaN/zero/constant cols â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "keep = ['cik']\n",
        "for c in wide.columns[1:]:\n",
        "    s = wide[c].dropna()\n",
        "    if s.empty or (s==0).all(): continue\n",
        "    if wide.shape[0] > 1 and s.nunique()==1: continue\n",
        "    keep.append(c)\n",
        "wide = wide[keep]\n",
        "print(\"After pruning useless cols:\", wide.shape)\n",
        "\n",
        "# â”€â”€ save â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "wide.to_csv(OUT, index=False)\n",
        "print(\"âœ… saved\", OUT)\n"
      ],
      "metadata": {
        "id": "nA2-ifFIHCnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "#  IF CLEAN & PRUNE  â†’  MISSINGNESS THRESHOLDS --NOT Currently USED\n",
        "# ================================================================\n",
        "!pip install tqdm --quiet\n",
        "\n",
        "import pandas as pd, numpy as np, sys\n",
        "from google.colab import files\n",
        "\n",
        "def pf(*args, **kwargs):\n",
        "    print(*args, **kwargs, flush=True)\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# CONFIGURATION\n",
        "# ----------------------------------------------------------------\n",
        "INPUT_PATH   = '/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/EODHD_roc_value_stats_ALL_CIK_flat_Qcols.csv'\n",
        "OUTPUT_PATH  = '/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/EODHD_roc_value_stats_ALL_CIK_flat_Qcols50.csv'\n",
        "\n",
        "COL_MISS_TH  = 0.50    # drop columns with >50% missing\n",
        "ROW_MISS_TH  = 1   # drop rows   with >100% missing\n",
        "IMPUTE_FILL  = True    # if True, fill remaining NaNs with column median\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 1 â€¢ LOAD\n",
        "# ----------------------------------------------------------------\n",
        "df = pd.read_csv(INPUT_PATH, low_memory=False)\n",
        "pf(f\"STEP 1 â€¢ Loaded data: {df.shape[0]:,} rows Ã— {df.shape[1]:,} cols\")\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 2 â€¢ DROP COLUMNS BY MISSINGNESS\n",
        "# ----------------------------------------------------------------\n",
        "col_frac = df.isna().mean()\n",
        "cols_to_drop = col_frac[col_frac > COL_MISS_TH].index.tolist()\n",
        "pf(f\"STEP 2 â€¢ Columns > {COL_MISS_TH*100:.0f}% missing: {len(cols_to_drop)} will be dropped\")\n",
        "pf(\"    Examples:\", cols_to_drop[:10], \"â€¦\")\n",
        "df.drop(columns=cols_to_drop, inplace=True)\n",
        "pf(f\"STEP 2 â€¢ New shape: {df.shape[0]:,} rows Ã— {df.shape[1]:,} cols\")\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 3 â€¢ DROP ROWS BY MISSINGNESS\n",
        "# ----------------------------------------------------------------\n",
        "row_frac = df.isna().mean(axis=1)\n",
        "rows_to_drop = row_frac[row_frac > ROW_MISS_TH].index\n",
        "pf(f\"STEP 3 â€¢ Rows > {ROW_MISS_TH*100:.0f}% missing: {len(rows_to_drop):,} will be dropped\")\n",
        "df.drop(index=rows_to_drop, inplace=True)\n",
        "pf(f\"STEP 3 â€¢ New shape: {df.shape[0]:,} rows Ã— {df.shape[1]:,} cols\")\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 4 â€¢ OPTIONAL IMPUTATION\n",
        "# ----------------------------------------------------------------\n",
        "remain_miss = int(df.isna().sum().sum())\n",
        "pf(f\"STEP 4 â€¢ Remaining missing values: {remain_miss:,}\")\n",
        "if IMPUTE_FILL and remain_miss > 0:\n",
        "    medians = df.median(numeric_only=True)\n",
        "    df.fillna(medians, inplace=True)\n",
        "    pf(\"STEP 4 â€¢ Filled remaining NaNs with column medians\")\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# STEP 5 â€¢ SAVE & DOWNLOAD\n",
        "# ----------------------------------------------------------------\n",
        "df.to_csv(OUTPUT_PATH, index=False)\n",
        "pf(f\"âœ…  Cleaned data saved to: {OUTPUT_PATH}  ({df.shape[0]:,}Ã—{df.shape[1]:,})\")\n",
        "files.download(OUTPUT_PATH)\n"
      ],
      "metadata": {
        "id": "R7prAkRpJ1pS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Load both files\n",
        "df_roc   = pd.read_csv('/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/EODHD_roc_value_stats_ALL_CIK_flat_Qcols50.csv')\n",
        "df_macro = pd.read_csv('/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/EODHD_adjusted_macrosnp_joined_with_metadata_incorp_wsf.csv')\n",
        "\n",
        "# 2. Inspect shapes\n",
        "print(\"ROC file shape:  \", df_roc.shape)\n",
        "print(\"Macro file shape:\", df_macro.shape)\n",
        "\n",
        "# 3. Find common columns\n",
        "common = list(set(df_roc.columns).intersection(df_macro.columns))\n",
        "print(\"Common columns:\\n\", common)\n",
        "\n",
        "# â†’ pick your joinâ€keys from that list.\n",
        "#    e.g. ticker + quarter/date column, or whatever makes sense for your data.\n",
        "#    For demonstration, let's assume you want to join on 'ticker' and 'fyearq'\n",
        "join_keys = ['cik']  # â† replace with your actual keys\n",
        "\n",
        "# 4. Perform the inner join\n",
        "df_merged = pd.merge(\n",
        "    df_roc,\n",
        "    df_macro,\n",
        "    on=join_keys,\n",
        "    how='inner',\n",
        "    validate='one_to_many'  # optional sanity check: change if necessary\n",
        ")\n",
        "\n",
        "print(\"Merged shape:\", df_merged.shape)\n",
        "\n",
        "# 5. (Optional) inspect a few rows\n",
        "print(df_merged.head())\n",
        "\n",
        "# 6. Save back to CSV\n",
        "output_path = '/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/EODHD_Final_cik50_flat_wsf_incorp.csv'\n",
        "df_merged.to_csv(output_path, index=False)\n",
        "print(\"Saved merged file to:\", output_path)\n"
      ],
      "metadata": {
        "id": "wcR6w7ayKA4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('EODHD_Final_cik50_flat_wsf_incorp.csv')\n",
        "\n",
        "# Convert start_date to datetime\n",
        "df['start_date'] = pd.to_datetime(df['start_date'], errors='coerce')\n",
        "\n",
        "# Extract just the month in MM format\n",
        "df['start_month'] = df['start_date'].dt.strftime('%m')\n",
        "\n",
        "# Preview the result\n",
        "print(df[['start_date', 'start_month']].head())\n",
        "\n",
        "# Save to a new CSV (optional)\n",
        "df.to_csv('EODHD_Final_cik50_flat_wsf_incorp_m.csv', index=False)\n"
      ],
      "metadata": {
        "id": "NKnoMrbcKDaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modeling ran on SOL computer\n",
        "\n",
        "# ============================================================\n",
        "#  LOW-RAM GPU/CPU TRAINING  â€“ SOL / scratch/ghtillem\n",
        "#  â€¢ numeric down-cast\n",
        "#  â€¢ sparse One-Hot\n",
        "#  â€¢ 3-fold CV, 20 Optuna trials\n",
        "# ============================================================\n",
        "\n",
        "import os, gc, warnings, joblib, numpy as np, pandas as pd, optuna\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import roc_auc_score, make_scorer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import lightgbm as lgb, xgboost as xgb\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ---------- CONFIG ---------------------------------------------------------\n",
        "DATA_FILE = \"/scratch/ghtillem/EODHD_Final_maxcik50_flat_wsf_incorp_m.csv\"\n",
        "SAVE_DIR  = \"/scratch/ghtillem/Saved_Models\"\n",
        "TARGETS   = ['flag_6m_price', 'flag_3y_price', 'flag_6m_avg', 'flag_3y_avg']\n",
        "TEST_SIZE = 0.30\n",
        "SEED      = 42\n",
        "N_TRIALS  = 20     # fewer trials -> less RAM\n",
        "FOLDS     = 3      # 3-fold CV\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "LEAKAGE = [  # full list\n",
        "    'six_month_price','three_year_price','six_month_30d_avg','three_year_30d_avg',\n",
        "    'flag_6m_price','flag_3y_price','flag_6m_avg','flag_3y_avg',\n",
        "    'six_month_date','three_year_date','sp500_sma30_centered_at_start',\n",
        "    'sp500_above_30sma_centered','_merge','cik','title','1Y_zipma_ROC',\n",
        "    'All_zipba_ROC','1Y_zipba_ROC','All_sic_ROC','1Y_fye_ROC','1Y_sic_ROC',\n",
        "    '3Y_zipma_ROC','3Y_zipba_ROC','All_median_zipba','Q1_zipba_median',\n",
        "    'Q1_sic_mean_y','Q4_sic_median_y','Q1_zipma_median_y','Q2_sic_mean_x',\n",
        "    'Q2_sic_median','Q0_sic_mean','Q0_zipba_mean','sic','Q1_zipma_median_x',\n",
        "    'Q1_zipma_mean_x','Q1_fye_median_x','Q0_fye_mean','Q1_zipba_median_y',\n",
        "    'Q0_zipma_ROC_x','Q4_sic_mean_y','Q0_zipba_mean_y','Q0_zipba_mean_x',\n",
        "    'Q4_zipma_median_x','Q2_fye_mean_x','bus_zip','Q4_fye_median',\n",
        "    'Q2_fye_mean_y','Q1_fye_mean','Q0_zipma_median_y','Q1_fye_median_y',\n",
        "    'Q0_fye_median_y'\n",
        "]\n",
        "\n",
        "ID_COLS = ['ticker','company_name','phone','adsh','instance']\n",
        "\n",
        "# ---------- 1. LOAD & DOWN-CAST -------------------------------------------\n",
        "df = pd.read_csv(DATA_FILE, low_memory=False)\n",
        "\n",
        "for col in df.select_dtypes(\"float\"):\n",
        "    df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
        "for col in df.select_dtypes(\"integer\"):\n",
        "    df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
        "\n",
        "for b in ('sp500_above_50sma','sp500_above_200sma'):\n",
        "    if b in df: df[b] = df[b].fillna(False).astype(int)\n",
        "\n",
        "# ---------- 2. PREPROCESSOR -----------------------------------------------\n",
        "def make_preprocessor(X):\n",
        "    num = X.select_dtypes(\"number\").columns.tolist()\n",
        "    cat = X.select_dtypes(\"object\").columns.tolist()\n",
        "    return ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", SimpleImputer(strategy=\"median\"), num),\n",
        "            (\"cat\", Pipeline([\n",
        "                (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
        "                (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse=True))\n",
        "            ]), cat)\n",
        "        ],\n",
        "        sparse_threshold=0.3  # keep whole output sparse if >=30 % zeros\n",
        "    )\n",
        "\n",
        "# ---------- 3. MODEL BUILDERS (smaller) -----------------------------------\n",
        "def build_lgb(t):\n",
        "    return lgb.LGBMClassifier(\n",
        "        device_type=\"gpu\",\n",
        "        n_estimators   = t.suggest_int (\"n\",     200, 800),\n",
        "        num_leaves     = t.suggest_int (\"leaves\", 15, 128),\n",
        "        max_depth      = t.suggest_int (\"depth\", -1, 6),\n",
        "        max_bin        = 255,\n",
        "        learning_rate  = t.suggest_float(\"lr\", 3e-3, .15, log=True),\n",
        "        subsample      = t.suggest_float(\"sub\", .6, 1.0),\n",
        "        colsample_bytree=t.suggest_float(\"col\", .6, 1.0),\n",
        "        random_state=SEED, n_jobs=4\n",
        "    )\n",
        "\n",
        "def build_xgb(t):\n",
        "    return xgb.XGBClassifier(\n",
        "        tree_method=\"gpu_hist\", predictor=\"gpu_predictor\",\n",
        "        n_estimators   = t.suggest_int (\"n\",    200, 800),\n",
        "        max_depth      = t.suggest_int (\"depth\",3, 6),\n",
        "        max_bin        = 256,\n",
        "        learning_rate  = t.suggest_float(\"lr\", 3e-3, .15, log=True),\n",
        "        subsample      = t.suggest_float(\"sub\", .6, 1.0),\n",
        "        colsample_bytree=t.suggest_float(\"col\", .6, 1.0),\n",
        "        reg_lambda     = t.suggest_float(\"l2\", 1e-3, 5, log=True),\n",
        "        reg_alpha      = t.suggest_float(\"l1\", 1e-3, 5, log=True),\n",
        "        random_state=SEED, n_jobs=4\n",
        "    )\n",
        "\n",
        "# ---------- 4. TRAIN LOOP --------------------------------------------------\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "AUC   = make_scorer(roc_auc_score, needs_proba=True)\n",
        "CV    = StratifiedKFold(FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "for target in TARGETS:\n",
        "    print(f\"\\nâ•â•â•â• {target} â•â•â•â•\")\n",
        "    y = df[target].astype(int)\n",
        "    X = (df\n",
        "         .drop(columns=LEAKAGE + [target] + ID_COLS, errors=\"ignore\")\n",
        "         .assign(**{c: df[c].astype(str) for c in df.select_dtypes(\"object\")})\n",
        "    )\n",
        "\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "        X, y, test_size=TEST_SIZE, stratify=y, random_state=SEED)\n",
        "    pre = make_preprocessor(X_tr)\n",
        "\n",
        "    def objective(trial):\n",
        "        algo = trial.suggest_categorical(\"algo\", [\"lgb\", \"xgb\"])\n",
        "        clf  = build_lgb(trial) if algo==\"lgb\" else build_xgb(trial)\n",
        "        pipe = Pipeline([(\"prep\", pre), (\"clf\", clf)])\n",
        "        score = cross_val_score(pipe, X_tr, y_tr, cv=CV,\n",
        "                                scoring=AUC, n_jobs=1).mean()\n",
        "        return 0.5 if np.isnan(score) else score\n",
        "\n",
        "    study = optuna.create_study(direction=\"maximize\",\n",
        "                                sampler=optuna.samplers.TPESampler(seed=SEED))\n",
        "    study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
        "\n",
        "    params = study.best_trial.params; algo = params.pop(\"algo\")\n",
        "    best_clf = build_lgb(optuna.trial.FixedTrial(params)) if algo==\"lgb\" else \\\n",
        "               build_xgb(optuna.trial.FixedTrial(params))\n",
        "\n",
        "    pipe = Pipeline([(\"prep\", pre), (\"clf\", best_clf)])\n",
        "    pipe.fit(X_tr, y_tr)\n",
        "\n",
        "    auc = roc_auc_score(y_te, pipe.predict_proba(X_te)[:,1])\n",
        "    print(f\"  â€¢ test AUC = {auc:.4f}  |  {algo.upper()}\")\n",
        "\n",
        "    path = f\"{SAVE_DIR}/{target}_lowRAM.pkl\"\n",
        "    joblib.dump(pipe, path, compress=3)\n",
        "    print(\"  ğŸ’¾ saved\", path)\n",
        "\n",
        "    del pipe, pre, best_clf; gc.collect()\n",
        "\n",
        "print(\"\\nâœ… Finished.\")\n"
      ],
      "metadata": {
        "id": "iXpissg4KAxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NN model ran on SOL\n",
        "\n",
        "import os\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
        "        print(f\"Enabled TF GPU memory growth for {len(gpus)} GPU(s).\")\n",
        "    else:\n",
        "        print(\"No GPU detected by TensorFlow.\")\n",
        "except ImportError:\n",
        "    print(\"TensorFlow not found, skipping GPU memory growth setting.\")\n",
        "\n",
        "import json\n",
        "import warnings\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.utils import class_weight\n",
        "import optuna\n",
        "\n",
        "# --- CONFIG ---\n",
        "DATA_FILE         = '/scratch/ghtillem/Data/EODHD_Final_maxcik50_flat_wsf_incorp_m.csv'\n",
        "OUTPUT_DIR        = '/scratch/ghtillem/Saved_NN_Models/final/final'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "TEST_SIZE         = 0.30\n",
        "VAL_FOLDS         = 5\n",
        "RANDOM_SEED       = 42\n",
        "BATCH_SIZE        = 512\n",
        "MAX_EPOCHS        = 100\n",
        "EARLY_PATIENCE    = 25\n",
        "N_TRIALS          = 100 # Increased for proper hyperparameter search\n",
        "CLEAN_OLD_STUDIES = False\n",
        "\n",
        "TARGETS = ['flag_3y_avg', 'flag_6m_avg']\n",
        "LEAKAGE = [\n",
        "    'six_month_price', 'three_year_price', 'six_month_30d_avg', 'three_year_30d_avg',\n",
        "    'flag_6m_price', 'flag_3y_price', 'six_month_date', 'three_year_date',\n",
        "    'sp500_sma30_centered_at_start', 'sp500_above_30sma_centered', '_merge',\n",
        "    'cik', 'title', '1Y_zipma_ROC', 'All_zipba_ROC', '1Y_zipba_ROC', 'All_sic_ROC',\n",
        "    '1Y_fye_ROC', '1Y_sic_ROC', '3Y_zipma_ROC', '3Y_zipba_ROC', 'All_median_zipba',\n",
        "    'Q1_zipba_median', 'Q1_sic_mean_y', 'Q4_sic_median_y', 'Q1_zipma_median_y',\n",
        "    'Q2_sic_mean_x', 'Q2_sic_median', 'Q0_sic_mean', 'Q0_zipba_mean', 'sic',\n",
        "    'Q1_zipma_median_x', 'Q1_zippa_mean_x', 'Q1_fye_median_x', 'Q0_fye_mean',\n",
        "    'Q1_zipba_median_y', 'Q0_zipma_ROC_x', 'Q4_sic_mean_y', 'Q0_zipba_mean_y',\n",
        "    'Q0_zipba_mean_x', 'Q4_zipma_median_x', 'Q2_fye_mean_x', 'bus_zip', 'Q4_fye_median',\n",
        "    'Q2_fye_mean_y', 'Q1_fye_mean', 'Q0_zipma_median_y', 'Q1_fye_median_y', 'Q0_fye_median_y',\n",
        "    'phone', 'ticker', 'start_date'\n",
        "]\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Preprocessing Function ---\n",
        "def build_preprocessor(df, target_cols_to_drop, include_pca=False, pca_n=None):\n",
        "    \"\"\"\n",
        "    Builds the preprocessing pipeline. Optionally includes PCA if specified.\n",
        "    Excludes leakage and specified target columns.\n",
        "    Returns the preprocessor and lists of numerical and categorical columns.\n",
        "    \"\"\"\n",
        "    cols_to_drop = set(LEAKAGE) | set(target_cols_to_drop)\n",
        "    df_features = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors='ignore')\n",
        "\n",
        "    feature_cols = df_features.columns.tolist()\n",
        "    if not feature_cols:\n",
        "        raise ValueError(\"No feature columns remaining after dropping leakage and targets.\")\n",
        "\n",
        "    num_cols = df_features.select_dtypes(include=np.number).columns.tolist()\n",
        "    cat_cols = df_features.select_dtypes(exclude=np.number).columns.tolist()\n",
        "    logger.debug(f\"Num cols identified: {len(num_cols)}\")\n",
        "    logger.debug(f\"Cat cols identified: {len(cat_cols)}\")\n",
        "\n",
        "    num_pipe = Pipeline([\n",
        "        ('impute', SimpleImputer(strategy='median')),\n",
        "        ('scale', StandardScaler())\n",
        "    ])\n",
        "    if include_pca and pca_n is not None and pca_n > 0:\n",
        "        num_pipe.steps.append(('pca', PCA(n_components=pca_n)))\n",
        "        logger.info(f\"Added PCA with n_components={pca_n} to numerical pipeline.\")\n",
        "\n",
        "    cat_pipe = Pipeline([\n",
        "        ('impute', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "        ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer([\n",
        "        ('num', num_pipe, num_cols),\n",
        "        ('cat', cat_pipe, cat_cols)\n",
        "    ], remainder='drop')\n",
        "\n",
        "    return preprocessor, num_cols, cat_cols\n",
        "\n",
        "# --- Save Preprocessing Metadata ---\n",
        "def save_preprocessing_metadata(output_dir, target, num_cols, cat_cols, preprocessor, timestamp):\n",
        "    \"\"\"\n",
        "    Saves preprocessing metadata (numerical and categorical columns, feature names after OHE).\n",
        "    \"\"\"\n",
        "    metadata = {\n",
        "        'numerical_columns': num_cols,\n",
        "        'categorical_columns': cat_cols,\n",
        "    }\n",
        "    try:\n",
        "        # Get feature names after one-hot encoding\n",
        "        ohe = preprocessor.named_transformers_['cat'].named_steps['ohe']\n",
        "        cat_feature_names = ohe.get_feature_names_out(metadata['categorical_columns']).tolist()\n",
        "        # Numerical feature names (after PCA if applicable)\n",
        "        if 'pca' in preprocessor.named_transformers_['num'].named_steps:\n",
        "            num_feature_names = [f'pca_{i}' for i in range(preprocessor.named_transformers_['num'].named_steps['pca'].n_components_)]\n",
        "        else:\n",
        "            num_feature_names = metadata['numerical_columns']\n",
        "        metadata['feature_names'] = num_feature_names + cat_feature_names\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not retrieve feature names: {e}\")\n",
        "        metadata['feature_names'] = []\n",
        "\n",
        "    metadata_path = os.path.join(output_dir, f\"{target}_preprocessor_metadata_{timestamp}.json\")\n",
        "    try:\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "        logger.info(f\"Saved preprocessing metadata to {metadata_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error saving preprocessing metadata: {e}\")\n",
        "\n",
        "# --- Load Data ---\n",
        "logger.info(f\"Loading data from {DATA_FILE}\")\n",
        "try:\n",
        "    df = pd.read_csv(DATA_FILE, low_memory=False)\n",
        "    logger.info(f\"Data loaded successfully. Shape: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    logger.error(f\"Data file not found at {DATA_FILE}\")\n",
        "    sys.exit(1)\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error loading data: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- Three-Way Split for Proper Validation ---\n",
        "logger.info(\"Performing three-way split (train/validation/test)...\")\n",
        "results = []\n",
        "all_best_params = {}\n",
        "\n",
        "for TARGET in TARGETS:\n",
        "    logger.info(f\"================ Processing Target: {TARGET} ================\")\n",
        "\n",
        "    # Drop rows where the current target is missing\n",
        "    df_t = df.dropna(subset=[TARGET]).copy()\n",
        "    if df_t.empty:\n",
        "        logger.warning(f\"No data remaining for target {TARGET} after dropping NaNs. Skipping.\")\n",
        "        continue\n",
        "    df_t.reset_index(drop=True, inplace=True)\n",
        "    y = df_t[TARGET].astype(int)\n",
        "    X = df_t.drop(columns=[TARGET])\n",
        "\n",
        "    other_targets = [t for t in TARGETS if t != TARGET]\n",
        "\n",
        "    # Three-way split: train + validation (for Optuna CV) + test\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=TEST_SIZE,\n",
        "        stratify=y,\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp,\n",
        "        test_size=0.2,  # 20% of remaining data for validation\n",
        "        stratify=y_temp,\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    X_trainval = X_temp  # Full train+val set for final training\n",
        "    y_trainval = y_temp\n",
        "    logger.info(f\"Train set size: {X_train.shape[0]}, Validation set size: {X_val.shape[0]}, Test set size: {X_test.shape[0]}\")\n",
        "\n",
        "    # Log class distribution\n",
        "    logger.info(f\"Class distribution in y_train for {TARGET}: {np.bincount(y_train)}\")\n",
        "    logger.info(f\"Class distribution in y_val for {TARGET}: {np.bincount(y_val)}\")\n",
        "    logger.info(f\"Class distribution in y_test for {TARGET}: {np.bincount(y_test)}\")\n",
        "\n",
        "    # --- Build Base Preprocessor ---\n",
        "    base_preprocessor, num_cols, cat_cols = build_preprocessor(X_trainval, other_targets, include_pca=False)\n",
        "    if not num_cols and not cat_cols:\n",
        "        logger.error(f\"No numerical or categorical columns identified for target {TARGET}. Check LEAKAGE list and data.\")\n",
        "        continue\n",
        "\n",
        "    # --- Optuna Hyperparameter Optimization ---\n",
        "    logger.info(f\"Starting Optuna hyperparameter search for {TARGET}...\")\n",
        "\n",
        "    def objective(trial):\n",
        "        max_pca_components = min(len(num_cols), 50) if num_cols else 0\n",
        "        pca_n = trial.suggest_int('pca_n', 0, max_pca_components) if max_pca_components > 0 else 0\n",
        "        n_layers = trial.suggest_int('n_layers', 1, 5)\n",
        "        units = trial.suggest_int('units', 32, 512, step=32)\n",
        "        dropout_rate = trial.suggest_float('dropout', 0.1, 0.5, step=0.1)\n",
        "        learning_rate = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
        "\n",
        "        trial_preprocessor, _, _ = build_preprocessor(\n",
        "            X_trainval, other_targets, include_pca=(pca_n > 0), pca_n=pca_n\n",
        "        )\n",
        "\n",
        "        skf = StratifiedKFold(n_splits=VAL_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
        "        fold_val_aucs = []\n",
        "        fold_num = 0\n",
        "        for train_i, val_i in skf.split(X_train, y_train):\n",
        "            fold_num += 1\n",
        "            logger.debug(f\"Trial {trial.number}, Fold {fold_num}/{VAL_FOLDS}\")\n",
        "            X_tr, X_val_fold = X_train.iloc[train_i], X_train.iloc[val_i]\n",
        "            y_tr, y_val_fold = y_train.iloc[train_i], y_train.iloc[val_i]\n",
        "\n",
        "            X_tr_p = trial_preprocessor.fit_transform(X_tr)\n",
        "            X_val_p = trial_preprocessor.transform(X_val_fold)\n",
        "\n",
        "            tf.keras.backend.clear_session()\n",
        "            inp = keras.Input(shape=(X_tr_p.shape[1],))\n",
        "            x = inp\n",
        "            for i in range(n_layers):\n",
        "                x = layers.Dense(units, activation='relu', name=f'dense_{i}')(x)\n",
        "                x = layers.BatchNormalization(name=f'bn_{i}')(x)\n",
        "                x = layers.Dropout(dropout_rate, name=f'dropout_{i}')(x)\n",
        "            out = layers.Dense(1, activation='sigmoid', name='output')(x)\n",
        "            model = keras.Model(inp, out)\n",
        "\n",
        "            model.compile(\n",
        "                optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=[keras.metrics.AUC(name='auc')]\n",
        "            )\n",
        "\n",
        "            try:\n",
        "                unique_classes = np.unique(y_tr)\n",
        "                if len(unique_classes) > 1:\n",
        "                    weights = class_weight.compute_class_weight(class_weight='balanced', classes=unique_classes, y=y_tr)\n",
        "                    class_weights_dict = dict(enumerate(weights))\n",
        "                else:\n",
        "                    logger.warning(f\"Fold {fold_num} - Only one class present. No class weights.\")\n",
        "                    class_weights_dict = None\n",
        "            except ValueError as e:\n",
        "                logger.warning(f\"Fold {fold_num} - Could not compute class weights: {e}.\")\n",
        "                class_weights_dict = None\n",
        "\n",
        "            early_stopping = keras.callbacks.EarlyStopping(\n",
        "                monitor='val_auc',\n",
        "                patience=EARLY_PATIENCE,\n",
        "                mode='max',\n",
        "                restore_best_weights=True\n",
        "            )\n",
        "\n",
        "            history = model.fit(\n",
        "                X_tr_p, y_tr,\n",
        "                validation_data=(X_val_p, y_val_fold),\n",
        "                epochs=MAX_EPOCHS,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                class_weight=class_weights_dict,\n",
        "                callbacks=[early_stopping],\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            best_val_auc = max(history.history['val_auc'])\n",
        "            fold_val_aucs.append(best_val_auc)\n",
        "\n",
        "        mean_val_auc = float(np.mean(fold_val_aucs))\n",
        "        logger.info(f\"Trial {trial.number} completed. Mean Validation AUC: {mean_val_auc:.4f}\")\n",
        "        return mean_val_auc\n",
        "\n",
        "    study_name = f\"optuna_nn_{TARGET}\"\n",
        "    storage_path = f\"sqlite:///{os.path.join(OUTPUT_DIR, study_name)}.db\"\n",
        "    logger.info(f\"Optuna study: {study_name}, Storage: {storage_path}\")\n",
        "\n",
        "    if CLEAN_OLD_STUDIES:\n",
        "        db_file = os.path.join(OUTPUT_DIR, f\"{study_name}.db\")\n",
        "        if os.path.exists(db_file):\n",
        "            logger.warning(f\"Removing existing Optuna study database: {db_file}\")\n",
        "            os.remove(db_file)\n",
        "\n",
        "    study = optuna.create_study(\n",
        "        study_name=study_name,\n",
        "        direction='maximize',\n",
        "        sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED),\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=VAL_FOLDS // 2),\n",
        "        storage=storage_path,\n",
        "        load_if_exists=not CLEAN_OLD_STUDIES\n",
        "    )\n",
        "\n",
        "    study.optimize(objective, n_trials=N_TRIALS, timeout=None)\n",
        "\n",
        "    study_pkl_path = os.path.join(OUTPUT_DIR, f\"{study_name}.pkl\")\n",
        "    joblib.dump(study, study_pkl_path)\n",
        "    logger.info(f\"Optuna study results saved to {study_pkl_path}\")\n",
        "\n",
        "    best_params = study.best_params\n",
        "    best_value = study.best_value\n",
        "    all_best_params[TARGET] = best_params\n",
        "    logger.info(f\"Best trial for {TARGET}: Value (Mean Val AUC): {best_value:.4f}\")\n",
        "    logger.info(f\"Best hyperparameters for {TARGET}: {best_params}\")\n",
        "\n",
        "    # --- Final Model Training & Saving ---\n",
        "    logger.info(f\"Starting final model training for {TARGET}...\")\n",
        "\n",
        "    # Build final preprocessor with PCA if selected by Optuna\n",
        "    best_pca_n = best_params.get('pca_n', 0)\n",
        "    final_preprocessor, num_cols_final, cat_cols_final = build_preprocessor(\n",
        "        X_trainval, other_targets, include_pca=(best_pca_n > 0), pca_n=best_pca_n\n",
        "    )\n",
        "    final_preprocessor.fit(X_trainval)\n",
        "\n",
        "    X_train_processed = final_preprocessor.transform(X_train)\n",
        "    X_val_processed = final_preprocessor.transform(X_val)\n",
        "    X_test_processed = final_preprocessor.transform(X_test)\n",
        "    logger.info(f\"Processed shapes - Train: {X_train_processed.shape}, Val: {X_val_processed.shape}, Test: {X_test_processed.shape}\")\n",
        "\n",
        "    # Save the final fitted preprocessor and metadata\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    prep_final_path = os.path.join(OUTPUT_DIR, f\"{TARGET}_preprocessor_final_{timestamp}.joblib\")\n",
        "    joblib.dump(final_preprocessor, prep_final_path)\n",
        "    logger.info(f\"Saved final fitted preprocessor to {prep_final_path}\")\n",
        "\n",
        "    save_preprocessing_metadata(OUTPUT_DIR, TARGET, num_cols_final, cat_cols_final, final_preprocessor, timestamp)\n",
        "\n",
        "    # Build and train the final model\n",
        "    tf.keras.backend.clear_session()\n",
        "    input_shape = (X_train_processed.shape[1],)\n",
        "    inp_f = keras.Input(shape=input_shape)\n",
        "    x_f = inp_f\n",
        "    best_n_layers = best_params.get('n_layers', 1)\n",
        "    best_units = best_params['units']\n",
        "    best_dropout = best_params['dropout']\n",
        "    best_lr = best_params['lr']\n",
        "\n",
        "    for i in range(best_n_layers):\n",
        "        x_f = layers.Dense(best_units, activation='relu', name=f'dense_{i}')(x_f)\n",
        "        x_f = layers.BatchNormalization(name=f'bn_{i}')(x_f)\n",
        "        x_f = layers.Dropout(best_dropout, name=f'dropout_{i}')(x_f)\n",
        "    out_f = layers.Dense(1, activation='sigmoid', name='output')(x_f)\n",
        "    final_model = keras.Model(inp_f, out_f)\n",
        "\n",
        "    final_model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=best_lr),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=[keras.metrics.AUC(name='auc')]\n",
        "    )\n",
        "    logger.info(\"Final model compiled.\")\n",
        "    final_model.summary(print_fn=logger.info)\n",
        "\n",
        "    try:\n",
        "        unique_classes_final = np.unique(y_train)\n",
        "        if len(unique_classes_final) > 1:\n",
        "            weights_final = class_weight.compute_class_weight(class_weight='balanced', classes=unique_classes_final, y=y_train)\n",
        "            class_weights_dict_final = dict(enumerate(weights_final))\n",
        "            logger.info(f\"Final class weights: {class_weights_dict_final}\")\n",
        "        else:\n",
        "            logger.warning(\"Only one class present in final training data.\")\n",
        "            class_weights_dict_final = None\n",
        "    except ValueError as e:\n",
        "        logger.warning(f\"Could not compute final class weights: {e}.\")\n",
        "        class_weights_dict_final = None\n",
        "\n",
        "    final_early_stopping = keras.callbacks.EarlyStopping(\n",
        "        monitor='val_auc',\n",
        "        patience=EARLY_PATIENCE,\n",
        "        mode='max',\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    final_model.fit(\n",
        "        X_train_processed, y_train,\n",
        "        validation_data=(X_val_processed, y_val),\n",
        "        epochs=MAX_EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_weight=class_weights_dict_final,\n",
        "        callbacks=[final_early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate on validation and test sets\n",
        "    logger.info(f\"Evaluating final model for {TARGET} on validation set...\")\n",
        "    val_loss, val_auc = final_model.evaluate(X_val_processed, y_val, verbose=0)\n",
        "    logger.info(f\"Validation Set Performance for {TARGET}: Loss = {val_loss:.4f}, AUC = {val_auc:.4f}\")\n",
        "\n",
        "    logger.info(f\"Evaluating final model for {TARGET} on test set...\")\n",
        "    test_loss, test_auc = final_model.evaluate(X_test_processed, y_test, verbose=0)\n",
        "    logger.info(f\"Test Set Performance for {TARGET}: Loss = {test_loss:.4f}, AUC = {test_auc:.4f}\")\n",
        "\n",
        "    # Save the final trained model\n",
        "    model_path = os.path.join(OUTPUT_DIR, f\"{TARGET}_model.keras\")\n",
        "    final_model.save(model_path)\n",
        "    logger.info(f\"Saved final trained model to {model_path}\")\n",
        "\n",
        "    results.append({\n",
        "        'target': TARGET,\n",
        "        'val_auc': float(val_auc),\n",
        "        'test_auc': float(test_auc)\n",
        "    })\n",
        "\n",
        "# --- Summary ---\n",
        "logger.info(\"NN training process complete for all targets.\")\n",
        "results_df = pd.DataFrame(results)\n",
        "summary_path = os.path.join(OUTPUT_DIR, 'summary_nn.csv')\n",
        "results_df.to_csv(summary_path, index=False)\n",
        "logger.info(f\"Summary of validation and test AUCs saved to {summary_path}\")\n",
        "print(\"\\n--- Final Validation and Test Set AUC Summary ---\")\n",
        "print(results_df)\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "logger.info(f\"Best hyperparameters found: {json.dumps(all_best_params, indent=2)}\")\n",
        "print(f\"NN training complete. Artifacts saved in: {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "TaChHQwUKHEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LightNGB ran on SOL\n",
        "\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import optuna\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import lightgbm as lgb\n",
        "\n",
        "# --- CONFIG ---\n",
        "DATA_FILE = '/scratch/ghtillem/Data/EODHD_Final_maxcik50_flat_wsf_incorp_m.csv'\n",
        "OUTPUT_DIR = '/scratch/ghtillem/Saved_LGBM_Models/Final/final'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "TEST_SIZE = 0.30\n",
        "VAL_FOLDS = 5\n",
        "VAL_REPEATS = 2\n",
        "RANDOM_SEED = 42\n",
        "LGBM_EARLY_STOPPING_ROUNDS = 50\n",
        "N_TRIALS = 100\n",
        "CLEAN_OLD_STUDIES = False\n",
        "\n",
        "TARGETS = ['flag_3y_avg', 'flag_6m_avg']\n",
        "LEAKAGE = [\n",
        "    'six_month_price', 'three_year_price', 'six_month_30d_avg', 'three_year_30d_avg',\n",
        "    'flag_6m_price', 'flag_3y_price', 'six_month_date', 'three_year_date',\n",
        "    'sp500_sma30_centered_at_start', 'sp500_above_30sma_centered', '_merge',\n",
        "    'cik', 'title', '1Y_zipma_ROC', 'All_zipba_ROC', '1Y_zipba_ROC', 'All_sic_ROC',\n",
        "    '1Y_fye_ROC', '1Y_sic_ROC', '3Y_zipma_ROC', '3Y_zipba_ROC', 'All_median_zipba',\n",
        "    'Q1_zipba_median', 'Q1_sic_mean_y', 'Q4_sic_median_y', 'Q1_zipma_median_y',\n",
        "    'Q2_sic_mean_x', 'Q2_sic_median', 'Q0_sic_mean', 'Q0_zipba_mean', 'sic',\n",
        "    'Q1_zipma_median_x', 'Q1_zippa_mean_x', 'Q1_fye_median_x', 'Q0_fye_mean',\n",
        "    'Q1_zipba_median_y', 'Q0_zipma_ROC_x', 'Q4_sic_mean_y', 'Q0_zipba_mean_y',\n",
        "    'Q0_zipba_mean_x', 'Q4_zipma_median_x', 'Q2_fye_mean_x', 'bus_zip', 'Q4_fye_median',\n",
        "    'Q2_fye_mean_y', 'Q1_fye_mean', 'Q0_zipma_median_y', 'Q1_fye_median_y', 'Q0_fye_median_y',\n",
        "    'phone', 'ticker', 'start_date'\n",
        "]\n",
        "\n",
        "# Setup logging\n",
        "timestamp_fmt = '%Y%m%d_%H%M%S'\n",
        "logging.basicConfig(\n",
        "    filename=os.path.join(OUTPUT_DIR, 'training.log'),\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger()\n",
        "\n",
        "# Detect GPU support\n",
        "device_type = 'cpu'\n",
        "try:\n",
        "    lgb.LGBMClassifier(device_type='gpu').fit(np.array([[1]]), [1])\n",
        "    device_type = 'gpu'\n",
        "    logger.info(\"GPU support confirmed.\")\n",
        "except Exception:\n",
        "    logger.info(\"GPU not available, using CPU.\")\n",
        "\n",
        "# --- Global Clip Outliers Function ---\n",
        "def clip_outliers(arr):\n",
        "    \"\"\"\n",
        "    Clips outliers in numerical data using the IQR method.\n",
        "    \"\"\"\n",
        "    q1, q3 = np.percentile(arr, [25, 75], axis=0)\n",
        "    iqr = q3 - q1\n",
        "    return np.clip(arr, q1 - 1.5 * iqr,  q3 + 1.5 * iqr)\n",
        "\n",
        "# --- Preprocessing Function ---\n",
        "def build_preprocessor(df, target_cols_to_drop):\n",
        "    \"\"\"\n",
        "    Builds the preprocessing pipeline with outlier clipping for numerical features.\n",
        "    Excludes leakage and specified target columns.\n",
        "    Returns the preprocessor, numerical columns, and categorical columns.\n",
        "    \"\"\"\n",
        "    cols_to_drop = set(LEAKAGE) | set(target_cols_to_drop)\n",
        "    df_features = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors='ignore')\n",
        "\n",
        "    feature_cols = df_features.columns.tolist()\n",
        "    if not feature_cols:\n",
        "        raise ValueError(\"No feature columns remaining after dropping leakage and targets.\")\n",
        "\n",
        "    num_cols = df_features.select_dtypes(include=np.number).columns.tolist()\n",
        "    cat_cols = df_features.select_dtypes(exclude=np.number).columns.tolist()\n",
        "    logger.info(f\"Identified {len(num_cols)} numerical and {len(cat_cols)} categorical columns.\")\n",
        "\n",
        "    num_pipe = Pipeline([\n",
        "        ('impute', SimpleImputer(strategy='median')),\n",
        "        ('clip', FunctionTransformer(clip_outliers, validate=False)),\n",
        "        ('scale', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    cat_pipe = Pipeline([\n",
        "        ('impute', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "        ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer([\n",
        "        ('num', num_pipe, num_cols),\n",
        "        ('cat', cat_pipe, cat_cols)\n",
        "    ], remainder='drop')\n",
        "\n",
        "    return preprocessor, num_cols, cat_cols\n",
        "\n",
        "# --- Save Preprocessing Metadata ---\n",
        "def save_preprocessing_metadata(output_dir, target, num_cols, cat_cols, preprocessor, timestamp):\n",
        "    \"\"\"\n",
        "    Saves preprocessing metadata (numerical and categorical columns, feature names after OHE).\n",
        "    \"\"\"\n",
        "    metadata = {\n",
        "        'numerical_columns': num_cols,\n",
        "        'categorical_columns': cat_cols,\n",
        "    }\n",
        "    try:\n",
        "        ohe = preprocessor.named_transformers_['cat'].named_steps['ohe']\n",
        "        cat_feature_names = ohe.get_feature_names_out(metadata['categorical_columns']).tolist()\n",
        "        metadata['feature_names'] = num_cols + cat_feature_names\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Could not retrieve feature names: {e}\")\n",
        "        metadata['feature_names'] = []\n",
        "\n",
        "    metadata_path = os.path.join(output_dir, f\"{target}_preprocessor_metadata_{timestamp}.json\")\n",
        "    try:\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "        logger.info(f\"Saved preprocessing metadata to {metadata_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error saving preprocessing metadata: {e}\")\n",
        "\n",
        "# --- Load Data ---\n",
        "logger.info(f\"Loading data from {DATA_FILE}\")\n",
        "try:\n",
        "    df = pd.read_csv(DATA_FILE, low_memory=False)\n",
        "    if df.empty:\n",
        "        raise ValueError(\"Dataframe empty.\")\n",
        "    missing = set(TARGETS) - set(df.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing target columns: {missing}\")\n",
        "    logger.info(f\"Data loaded: {df.shape}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error loading data: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- Drop Leakage Columns ---\n",
        "df.drop(columns=[c for c in LEAKAGE if c not in TARGETS], errors='ignore', inplace=True)\n",
        "\n",
        "results = []\n",
        "all_best_params = {}\n",
        "\n",
        "for TARGET in TARGETS:\n",
        "    logger.info(f\"================ Processing Target: {TARGET} ================\")\n",
        "    df_t = df.dropna(subset=[TARGET]).copy()\n",
        "    if df_t.empty:\n",
        "        logger.warning(f\"No data remaining for target {TARGET} after dropping NaNs. Skipping.\")\n",
        "        continue\n",
        "    df_t.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Convert bool to int\n",
        "    bool_cols = df_t.select_dtypes(include='bool').columns\n",
        "    df_t[bool_cols] = df_t[bool_cols].astype(int)\n",
        "\n",
        "    y = df_t[TARGET].astype(int)\n",
        "    X = df_t.drop(columns=[TARGET])\n",
        "\n",
        "    # Three-way split: train, validation, test\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_SEED\n",
        "    )\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.2, stratify=y_temp, random_state=RANDOM_SEED\n",
        "    )\n",
        "    X_trainval = X_temp\n",
        "    y_trainval = y_temp\n",
        "    logger.info(f\"Train set size: {X_train.shape[0]}, Validation set size: {X_val.shape[0]}, Test set size: {X_test.shape[0]}\")\n",
        "\n",
        "    # Log class distribution\n",
        "    logger.info(f\"Class distribution in y_train for {TARGET}: {np.bincount(y_train)}\")\n",
        "    logger.info(f\"Class distribution in y_val for {TARGET}: {np.bincount(y_val)}\")\n",
        "    logger.info(f\"Class distribution in y_test for {TARGET}: {np.bincount(y_test)}\")\n",
        "\n",
        "    # Build preprocessor\n",
        "    other_targets = [t for t in TARGETS if t != TARGET]\n",
        "    preprocessor, num_cols, cat_cols = build_preprocessor(X_trainval, other_targets)\n",
        "    preprocessor.fit(X_trainval)\n",
        "\n",
        "    # Transform data\n",
        "    X_train_p = preprocessor.transform(X_train)\n",
        "    X_val_p = preprocessor.transform(X_val)\n",
        "    X_test_p = preprocessor.transform(X_test)\n",
        "    cat_indices = list(range(len(num_cols), len(num_cols) + len(cat_cols)))\n",
        "    logger.info(f\"Processed shapes - Train: {X_train_p.shape}, Val: {X_val_p.shape}, Test: {X_test_p.shape}\")\n",
        "\n",
        "    # Save preprocessor and metadata\n",
        "    timestamp = datetime.now().strftime(timestamp_fmt)\n",
        "    prep_path = os.path.join(OUTPUT_DIR, f\"{TARGET}_preprocessor_{timestamp}.joblib\")\n",
        "    joblib.dump(preprocessor, prep_path)\n",
        "    logger.info(f\"Saved preprocessor to {prep_path}\")\n",
        "\n",
        "    save_preprocessing_metadata(OUTPUT_DIR, TARGET, num_cols, cat_cols, preprocessor, timestamp)\n",
        "\n",
        "    # Optuna objective\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            'objective': 'binary', 'metric': 'auc',\n",
        "            'random_state': RANDOM_SEED, 'n_jobs': -1,\n",
        "            'device_type': device_type,\n",
        "            'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart', 'goss']),\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 200, 2500, step=100),\n",
        "            'learning_rate': trial.suggest_categorical('learning_rate', [0.005, 0.01, 0.05, 0.1]),\n",
        "            'num_leaves': trial.suggest_int('num_leaves', 10, 150),\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10, log=True),\n",
        "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10, log=True),\n",
        "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
        "            'min_child_weight': trial.suggest_float('min_child_weight', 1e-3, 10, log=True),\n",
        "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
        "        }\n",
        "        skf = RepeatedStratifiedKFold(n_splits=VAL_FOLDS, n_repeats=VAL_REPEATS, random_state=RANDOM_SEED)\n",
        "        aucs = []\n",
        "        fold_num = 0\n",
        "        for train_idx, val_idx in skf.split(X_train, y_train):\n",
        "            fold_num += 1\n",
        "            Xt, Xv = X_train_p[train_idx], X_train_p[val_idx]\n",
        "            yt, yv = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "            params['scale_pos_weight'] = (yt == 0).sum() / (yt == 1).sum() if (yt == 1).sum() > 0 else 1.0\n",
        "            model = lgb.LGBMClassifier(**params)\n",
        "            callbacks = [lgb.early_stopping(LGBM_EARLY_STOPPING_ROUNDS)] if params['boosting_type'] != 'dart' else []\n",
        "            model.fit(\n",
        "                Xt, yt,\n",
        "                eval_set=[(Xv, yv)],\n",
        "                eval_metric='auc',\n",
        "                callbacks=callbacks,\n",
        "                categorical_feature=cat_indices\n",
        "            )\n",
        "            auc = roc_auc_score(yv, model.predict_proba(Xv)[:, 1])\n",
        "            aucs.append(auc)\n",
        "            trial.report(np.mean(aucs), fold_num)\n",
        "            if trial.should_prune():\n",
        "                logger.info(f\"Trial {trial.number} pruned.\")\n",
        "                raise optuna.exceptions.TrialPruned()\n",
        "        mean_auc = float(np.mean(aucs))\n",
        "        logger.info(f\"Trial {trial.number} completed. Mean CV AUC: {mean_auc:.4f}\")\n",
        "        return mean_auc\n",
        "\n",
        "    # Run Optuna\n",
        "    study_name = f\"optuna_lgbm_{TARGET}\"\n",
        "    db_path = os.path.join(OUTPUT_DIR, f\"{study_name}.db\")\n",
        "    if CLEAN_OLD_STUDIES and os.path.exists(db_path):\n",
        "        logger.warning(f\"Removing existing Optuna study database: {db_path}\")\n",
        "        os.remove(db_path)\n",
        "    study = optuna.create_study(\n",
        "        study_name=study_name,\n",
        "        direction='maximize',\n",
        "        sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED),\n",
        "        pruner=optuna.pruners.SuccessiveHalvingPruner(),\n",
        "        storage=f\"sqlite:///{db_path}\",\n",
        "        load_if_exists=not CLEAN_OLD_STUDIES\n",
        "    )\n",
        "    study.optimize(objective, n_trials=N_TRIALS)\n",
        "\n",
        "    # Save Optuna results\n",
        "    study_pkl_path = os.path.join(OUTPUT_DIR, f\"{study_name}.pkl\")\n",
        "    joblib.dump(study, study_pkl_path)\n",
        "    logger.info(f\"Saved Optuna study to {study_pkl_path}\")\n",
        "    pd.DataFrame(study.trials_dataframe()).to_csv(os.path.join(OUTPUT_DIR, f\"{study_name}_trials.csv\"), index=False)\n",
        "    with open(os.path.join(OUTPUT_DIR, f\"{study_name}_best.json\"), 'w') as f:\n",
        "        json.dump(study.best_params, f, indent=2)\n",
        "\n",
        "    # Final model training\n",
        "    logger.info(f\"Training final model for {TARGET} on full trainval set\")\n",
        "    final_params = study.best_params.copy()\n",
        "    final_params.update({\n",
        "        'objective': 'binary',\n",
        "        'metric': 'auc',\n",
        "        'random_state': RANDOM_SEED,\n",
        "        'n_jobs': -1,\n",
        "        'device_type': device_type,\n",
        "        'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum() if (y_train == 1).sum() > 0 else 1.0\n",
        "    })\n",
        "    final_model = lgb.LGBMClassifier(**final_params)\n",
        "    callbacks_final = [lgb.early_stopping(LGBM_EARLY_STOPPING_ROUNDS)] if final_params.get('boosting_type') != 'dart' else []\n",
        "    final_model.fit(\n",
        "        X_train_p, y_train,\n",
        "        eval_set=[(X_val_p, y_val)],\n",
        "        eval_metric='auc',\n",
        "        callbacks=callbacks_final,\n",
        "        categorical_feature=cat_indices\n",
        "    )\n",
        "\n",
        "    # Evaluate on validation and test sets\n",
        "    val_auc = roc_auc_score(y_val, final_model.predict_proba(X_val_p)[:, 1])\n",
        "    test_auc = roc_auc_score(y_test, final_model.predict_proba(X_test_p)[:, 1])\n",
        "    logger.info(f\"Validation AUC for {TARGET}: {val_auc:.4f}\")\n",
        "    logger.info(f\"Test AUC for {TARGET}: {test_auc:.4f}\")\n",
        "\n",
        "    # Save final artifacts\n",
        "    ts = datetime.now().strftime(timestamp_fmt)\n",
        "    model_file = os.path.join(OUTPUT_DIR, f\"{TARGET}_model_{ts}.txt\")\n",
        "    final_model.booster_.save_model(model_file)\n",
        "    logger.info(f\"Saved model to {model_file}\")\n",
        "\n",
        "    # Feature importance\n",
        "    fi = pd.DataFrame({\n",
        "        'feature': final_model.booster_.feature_name(),\n",
        "        'importance': final_model.feature_importances_\n",
        "    })\n",
        "    fi.sort_values('importance', ascending=False).to_csv(\n",
        "        os.path.join(OUTPUT_DIR, f\"{TARGET}_feature_importance.csv\"), index=False\n",
        "    )\n",
        "\n",
        "    results.append({\n",
        "        'target': TARGET,\n",
        "        'best_cv_auc': study.best_value,\n",
        "        'val_auc': val_auc,\n",
        "        'test_auc': test_auc})\n",
        "\n",
        "# Save summary\n",
        "results_df = pd.DataFrame(results)\n",
        "summary_path = os.path.join(OUTPUT_DIR, 'summary_lgbm_results.csv')\n",
        "results_df.to_csv(summary_path, index=False)\n",
        "logger.info(f\"Saved summary to {summary_path}\")\n",
        "print(\"\\n--- Final Validation and Test AUC Summary ---\")\n",
        "print(results_df)\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "logger.info(f\"Best hyperparameters found: {json.dumps(all_best_params, indent=2)}\")\n",
        "print(f\"LGBM training complete. Artifacts saved in: {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "W6lL6RoRKSz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Simulation ran on SOL\n",
        "\n",
        "#df_test is 30% split with randomseed = 42 using  EODHD_Final_maxcik50_flat_wsf_incorp_m.csv\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# â”€â”€â”€ 5) Simulation / Backtest â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "logger.info(\"Starting backtest simulation...\")\n",
        "\n",
        "# --- Simulation Parameters ---\n",
        "initial_capital = 100_000.0\n",
        "investment_per_signal = 100.0\n",
        "investment_both_signals = 200.0\n",
        "prediction_threshold = 0.6\n",
        "\n",
        "# --- Initialize Strategy Capitals ---\n",
        "rnd_cap = initial_capital\n",
        "mdl_cap = initial_capital\n",
        "nn_cap = initial_capital\n",
        "ens_cap = initial_capital\n",
        "\n",
        "# --- Initialize Cumulative Profits and Metrics Tracking ---\n",
        "rnd_cum = 0.0\n",
        "mdl_cum = 0.0\n",
        "nn_cum = 0.0\n",
        "ens_cum = 0.0\n",
        "\n",
        "# Metrics tracking\n",
        "rnd_wins = 0\n",
        "rnd_losses = 0\n",
        "rnd_win_profits = []\n",
        "rnd_loss_profits = []\n",
        "rnd_cum_profits = [0.0]  # For max drawdown\n",
        "rnd_investments = []\n",
        "\n",
        "mdl_wins = 0\n",
        "mdl_losses = 0\n",
        "mdl_win_profits = []\n",
        "mdl_loss_profits = []\n",
        "mdl_cum_profits = [0.0]\n",
        "mdl_investments = []\n",
        "mdl_correct_6m = 0\n",
        "mdl_correct_3y = 0\n",
        "mdl_predictions_6m = 0\n",
        "mdl_predictions_3y = 0\n",
        "\n",
        "nn_wins = 0\n",
        "nn_losses = 0\n",
        "nn_win_profits = []\n",
        "nn_loss_profits = []\n",
        "nn_cum_profits = [0.0]\n",
        "nn_investments = []\n",
        "nn_correct_6m = 0\n",
        "nn_correct_3y = 0\n",
        "nn_predictions_6m = 0\n",
        "nn_predictions_3y = 0\n",
        "\n",
        "ens_wins = 0\n",
        "ens_losses = 0\n",
        "ens_win_profits = []\n",
        "ens_loss_profits = []\n",
        "ens_cum_profits = [0.0]\n",
        "ens_investments = []\n",
        "ens_predictions_6m = 0\n",
        "ens_predictions_3y = 0\n",
        "ens_correct_6m     = 0\n",
        "ens_correct_3y     = 0\n",
        "\n",
        "# --- History Tracking ---\n",
        "history = []\n",
        "processed_rows = 0\n",
        "skipped_rows = 0\n",
        "\n",
        "# --- Verify Prediction Arrays ---\n",
        "try:\n",
        "    assert len(y_prob_6m) == len(df_test), f\"y_prob_6m length mismatch: {len(y_prob_6m)} vs {len(df_test)}\"\n",
        "    assert len(y_prob_3y) == len(df_test), f\"y_prob_3y length mismatch: {len(y_prob_3y)} vs {len(df_test)}\"\n",
        "    assert len(y_prob_nn_6m) == len(df_test), f\"y_prob_nn_6m length mismatch: {len(y_prob_nn_6m)} vs {len(df_test)}\"\n",
        "    assert len(y_prob_nn_3y) == len(df_test), f\"y_prob_nn_3y length mismatch: {len(y_prob_nn_3y)} vs {len(df_test)}\"\n",
        "    logger.info(\"All required prediction arrays found. Proceeding with simulation.\")\n",
        "except AssertionError as e:\n",
        "    logger.error(f\"Prediction array length validation failed: {e}\")\n",
        "    logger.error(traceback.format_exc())\n",
        "    raise RuntimeError(\"Prediction array validation failed\") from e\n",
        "\n",
        "# --- Simulation Loop ---\n",
        "try:\n",
        "    for i in range(len(df_test)):\n",
        "        true_6m = y_true_6m[i]\n",
        "        true_3y = y_true_3y[i]\n",
        "        sp = start_prices[i]\n",
        "        sp6 = sixm_prices[i]\n",
        "        sp3 = threey_prices[i]\n",
        "        ret_6m = p6m[i]\n",
        "        ret_3y = p3y[i]\n",
        "\n",
        "        # --- Data Validation ---\n",
        "        if (pd.isna(true_6m) or pd.isna(true_3y) or\n",
        "                pd.isna(sp) or sp <= 0 or\n",
        "                pd.isna(sp6) or sp6 <= 0 or\n",
        "                pd.isna(sp3) or sp3 <= 0 or\n",
        "                pd.isna(ret_6m) or pd.isna(ret_3y) or\n",
        "                ret_6m < percentile_6m_lower or ret_6m > percentile_6m_upper or\n",
        "                ret_3y < percentile_3y_lower or ret_3y > percentile_3y_upper):\n",
        "            skipped_rows += 1\n",
        "            logger.debug(f\"Skipping row {i}: Invalid data (flags, prices, or returns).\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            shares_per_dollar = 1.0 / sp\n",
        "        except ZeroDivisionError:\n",
        "            skipped_rows += 1\n",
        "            logger.warning(f\"Skipping row {i}: ZeroDivisionError calculating shares_per_dollar (sp={sp}).\")\n",
        "            continue\n",
        "\n",
        "        # --- Random Agent Decision ---\n",
        "        r6 = 1 if random.random() < 0.3 else 0\n",
        "        r3 = 1 if random.random() < 0.2 else 0\n",
        "        random_desired_invest = 0\n",
        "        if r6 and r3:\n",
        "            random_desired_invest = investment_both_signals\n",
        "        elif r6 or r3:\n",
        "            random_desired_invest = investment_per_signal\n",
        "\n",
        "        random_invest = 0\n",
        "        random_profit_this_ipo = 0\n",
        "        if rnd_cap >= random_desired_invest and random_desired_invest > 0:\n",
        "            random_invest = random_desired_invest\n",
        "            rnd_cap -= random_invest\n",
        "            shares = random_invest * shares_per_dollar\n",
        "            random_sold = 0\n",
        "            if random_invest == investment_per_signal and r6 == 1:\n",
        "                sold_val = shares * sp6\n",
        "                if pd.isna(sold_val):\n",
        "                    sold_val = shares * sp\n",
        "                random_sold = sold_val\n",
        "            elif random_invest == investment_per_signal and r3 == 1:\n",
        "                sold_val = shares * sp3\n",
        "                if pd.isna(sold_val):\n",
        "                    sold_val = shares * sp\n",
        "                random_sold = sold_val\n",
        "            elif random_invest == investment_both_signals:\n",
        "                half_shares = shares / 2\n",
        "                sold_6m_val = half_shares * sp6\n",
        "                sold_3y_val = half_shares * sp3\n",
        "                if pd.isna(sold_6m_val):\n",
        "                    sold_6m_val = half_shares * sp\n",
        "                if pd.isna(sold_3y_val):\n",
        "                    sold_3y_val = half_shares * sp\n",
        "                random_sold = sold_6m_val + sold_3y_val\n",
        "            rnd_cap += random_sold\n",
        "            random_profit_this_ipo = random_sold - random_invest\n",
        "            # Track metrics\n",
        "            if random_profit_this_ipo > 0:\n",
        "                rnd_wins += 1\n",
        "                rnd_win_profits.append(random_profit_this_ipo)\n",
        "            elif random_profit_this_ipo < 0:\n",
        "                rnd_losses += 1\n",
        "                rnd_loss_profits.append(random_profit_this_ipo)\n",
        "            rnd_cum += random_profit_this_ipo\n",
        "            rnd_cum_profits.append(rnd_cum)\n",
        "            rnd_investments.append(random_invest)\n",
        "\n",
        "        # --- LGBM Agent Decision ---\n",
        "        m6 = 1 if y_prob_6m[i] >= prediction_threshold else 0\n",
        "        m3 = 1 if y_prob_3y[i] >= prediction_threshold else 0\n",
        "        model_desired_invest = 0\n",
        "        if m6 and m3:\n",
        "            model_desired_invest = investment_both_signals\n",
        "        elif m6 or m3:\n",
        "            model_desired_invest = investment_per_signal\n",
        "\n",
        "        model_invest = 0\n",
        "        model_profit_this_ipo = 0\n",
        "        if mdl_cap >= model_desired_invest and model_desired_invest > 0:\n",
        "            model_invest = model_desired_invest\n",
        "            mdl_cap -= model_invest\n",
        "            shares = model_invest * shares_per_dollar\n",
        "            model_sold = 0\n",
        "            if model_invest == investment_per_signal and m6 == 1:\n",
        "                sold_val = shares * sp6\n",
        "                if pd.isna(sold_val):\n",
        "                    sold_val = shares * sp\n",
        "                model_sold = sold_val\n",
        "            elif model_invest == investment_per_signal and m3 == 1:\n",
        "                sold_val = shares * sp3\n",
        "                if pd.isna(sold_val):\n",
        "                    sold_val = shares * sp\n",
        "                model_sold = sold_val\n",
        "            elif model_invest == investment_both_signals:\n",
        "                half_shares = shares / 2\n",
        "                sold_6m_val = half_shares * sp6\n",
        "                sold_3y_val = half_shares * sp3\n",
        "                if pd.isna(sold_6m_val):\n",
        "                    sold_6m_val = half_shares * sp\n",
        "                if pd.isna(sold_3y_val):\n",
        "                    sold_3y_val = half_shares * sp\n",
        "                model_sold = sold_6m_val + sold_3y_val\n",
        "            mdl_cap += model_sold\n",
        "            model_profit_this_ipo = model_sold - model_invest\n",
        "            # Track metrics\n",
        "            if model_profit_this_ipo > 0:\n",
        "                mdl_wins += 1\n",
        "                mdl_win_profits.append(model_profit_this_ipo)\n",
        "            elif model_profit_this_ipo < 0:\n",
        "                mdl_losses += 1\n",
        "                mdl_loss_profits.append(model_profit_this_ipo)\n",
        "            mdl_cum += model_profit_this_ipo\n",
        "            mdl_cum_profits.append(mdl_cum)\n",
        "            mdl_investments.append(model_invest)\n",
        "            # Track prediction accuracy\n",
        "            if m6 == 1:\n",
        "                mdl_predictions_6m += 1\n",
        "                if int(true_6m) == 1:\n",
        "                    mdl_correct_6m += 1\n",
        "            if m3 == 1:\n",
        "                mdl_predictions_3y += 1\n",
        "                if int(true_3y) == 1:\n",
        "                    mdl_correct_3y += 1\n",
        "\n",
        "        # --- Neural Network Agent Decision ---\n",
        "        n6 = 1 if y_prob_nn_6m[i] >= prediction_threshold else 0\n",
        "        n3 = 1 if y_prob_nn_3y[i] >= prediction_threshold else 0\n",
        "        nn_desired_invest = 0\n",
        "        if n6 and n3:\n",
        "            nn_desired_invest = investment_both_signals\n",
        "        elif n6 or n3:\n",
        "            nn_desired_invest = investment_per_signal\n",
        "\n",
        "        nn_invest = 0\n",
        "        nn_profit_this_ipo = 0\n",
        "        if nn_cap >= nn_desired_invest and nn_desired_invest > 0:\n",
        "            nn_invest = nn_desired_invest\n",
        "            nn_cap -= nn_invest\n",
        "            shares = nn_invest * shares_per_dollar\n",
        "            nn_sold = 0\n",
        "            if nn_invest == investment_per_signal and n6 == 1:\n",
        "                sold_val = shares * sp6\n",
        "                if pd.isna(sold_val):\n",
        "                    sold_val = shares * sp\n",
        "                nn_sold = sold_val\n",
        "            elif nn_invest == investment_per_signal and n3 == 1:\n",
        "                sold_val = shares * sp3\n",
        "                if pd.isna(sold_val):\n",
        "                    sold_val = shares * sp\n",
        "                nn_sold = sold_val\n",
        "            elif nn_invest == investment_both_signals:\n",
        "                half_shares = shares / 2\n",
        "                sold_6m_val = half_shares * sp6\n",
        "                sold_3y_val = half_shares * sp3\n",
        "                if pd.isna(sold_6m_val):\n",
        "                    sold_6m_val = half_shares * sp\n",
        "                if pd.isna(sold_3y_val):\n",
        "                    sold_3y_val = half_shares * sp\n",
        "                nn_sold = sold_6m_val + sold_3y_val\n",
        "            nn_cap += nn_sold\n",
        "            nn_profit_this_ipo = nn_sold - nn_invest\n",
        "            # Track metrics\n",
        "            if nn_profit_this_ipo > 0:\n",
        "                nn_wins += 1\n",
        "                nn_win_profits.append(nn_profit_this_ipo)\n",
        "            elif nn_profit_this_ipo < 0:\n",
        "                nn_losses += 1\n",
        "                nn_loss_profits.append(nn_profit_this_ipo)\n",
        "            nn_cum += nn_profit_this_ipo\n",
        "            nn_cum_profits.append(nn_cum)\n",
        "            nn_investments.append(nn_invest)\n",
        "            # Track prediction accuracy\n",
        "            if n6 == 1:\n",
        "                nn_predictions_6m += 1\n",
        "                if int(true_6m) == 1:\n",
        "                    nn_correct_6m += 1\n",
        "            if n3 == 1:\n",
        "                nn_predictions_3y += 1\n",
        "                if int(true_3y) == 1:\n",
        "                    nn_correct_3y += 1\n",
        "\n",
        "                    # Ensemble Agent (avg of LGBM & NN)\n",
        "            e6 = 1 if (y_prob_6m[i] + y_prob_nn_6m[i]) / 2 >= prediction_threshold else 0\n",
        "            e3 = 1 if (y_prob_3y[i] + y_prob_nn_3y[i]) / 2 >= prediction_threshold else 0\n",
        "\n",
        "            ens_desired_invest = 0\n",
        "            if e6 and e3:\n",
        "                ens_desired_invest = investment_both_signals\n",
        "            elif e6 or e3:\n",
        "                ens_desired_invest = investment_per_signal\n",
        "\n",
        "            ens_invest = 0\n",
        "            ens_profit_this_ipo = 0\n",
        "            if ens_cap >= ens_desired_invest and ens_desired_invest > 0:\n",
        "                ens_invest = ens_desired_invest\n",
        "                ens_cap -= ens_invest\n",
        "                shares = ens_desired_invest * shares_per_dollar\n",
        "\n",
        "                if ens_desired_invest == investment_per_signal:\n",
        "                    price = sp6 if e6 else sp3\n",
        "                    if pd.isna(price) or price <= 0:\n",
        "                        price = sp\n",
        "                    ens_sold = shares * price\n",
        "                else:\n",
        "                    half = shares / 2\n",
        "                    v6 = half * sp6 if not pd.isna(sp6) and sp6 > 0 else half * sp\n",
        "                    v3 = half * sp3 if not pd.isna(sp3) and sp3 > 0 else half * sp\n",
        "                    ens_sold = v6 + v3\n",
        "\n",
        "                ens_cap += ens_sold\n",
        "                ens_profit_this_ipo = ens_sold - ens_desired_invest\n",
        "\n",
        "                # track ensemble metrics\n",
        "                if ens_profit_this_ipo > 0:\n",
        "                    ens_wins += 1\n",
        "                    ens_win_profits.append(ens_profit_this_ipo)\n",
        "                elif ens_profit_this_ipo < 0:\n",
        "                    ens_losses += 1\n",
        "                    ens_loss_profits.append(ens_profit_this_ipo)\n",
        "                ens_cum += ens_profit_this_ipo\n",
        "                ens_cum_profits.append(ens_cum)\n",
        "                ens_investments.append(ens_invest)\n",
        "\n",
        "        # --- Save Record ---\n",
        "        current_cik = df_test.loc[i, 'cik'] if 'cik' in df_test.columns else 'N/A'\n",
        "        current_start_date = df_test.loc[i, 'start_date']\n",
        "        history.append({\n",
        "            'CIK': current_cik,\n",
        "            'start_date': current_start_date.strftime('%Y-%m-%d') if pd.notna(current_start_date) else None,\n",
        "            'start_price': sp if not pd.isna(sp) else np.nan,\n",
        "            'six_month_price': sp6 if not pd.isna(sp6) else np.nan,\n",
        "            'three_year_price': sp3 if not pd.isna(sp3) else np.nan,\n",
        "            'return_6m': ret_6m if not pd.isna(ret_6m) else np.nan,\n",
        "            'return_3y': ret_3y if not pd.isna(ret_3y) else np.nan,\n",
        "            'random_predict': f\"6M:{r6} 3Y:{r3}\",\n",
        "            'lgbm_predict': f\"6M:{m6} 3Y:{m3}\",\n",
        "            'nn_predict': f\"6M:{n6} 3Y:{n3}\",\n",
        "            'random_desired_invest': random_desired_invest,\n",
        "            'random_actual_invest': random_invest,\n",
        "            'random_profit_this_ipo': random_profit_this_ipo,\n",
        "            'random_cumulative_profit': rnd_cum,\n",
        "            'random_capital_after_ipo': rnd_cap,\n",
        "            'lgbm_desired_invest': model_desired_invest,\n",
        "            'lgbm_actual_invest': model_invest,\n",
        "            'lgbm_profit_this_ipo': model_profit_this_ipo,\n",
        "            'lgbm_cumulative_profit': mdl_cum,\n",
        "            'lgbm_capital_after_ipo': mdl_cap,\n",
        "            'nn_desired_invest': nn_desired_invest,\n",
        "            'nn_actual_invest': nn_invest,\n",
        "            'nn_profit_this_ipo': nn_profit_this_ipo,\n",
        "            'nn_cumulative_profit': nn_cum,\n",
        "            'nn_capital_after_ipo': nn_cap,\n",
        "            'ens_desired_invest': ens_desired_invest,\n",
        "            'ens_actual_invest': ens_invest,\n",
        "            'ens_profit_this_ipo': ens_profit_this_ipo,\n",
        "            'ens_cumulative_profit': ens_cum,\n",
        "            'ens_capital_after_ipo': ens_cap,\n",
        "            'lgbm_prob_6m': y_prob_6m[i],\n",
        "            'lgbm_prob_3y': y_prob_3y[i],\n",
        "            'nn_prob_6m': y_prob_nn_6m[i],\n",
        "            'nn_prob_3y': y_prob_nn_3y[i],\n",
        "            'true_6m': int(true_6m) if not pd.isna(true_6m) else np.nan,\n",
        "            'true_3y': int(true_3y) if not pd.isna(true_3y) else np.nan\n",
        "        })\n",
        "        processed_rows += 1\n",
        "\n",
        "        if (i + 1) % 1000 == 0:\n",
        "            logger.info(f\"Processed {i + 1}/{len(df_test)} simulation steps...\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error during simulation loop at index {i}: {e}\")\n",
        "    logger.error(traceback.format_exc())\n",
        "    raise RuntimeError(\"Simulation loop failed\") from e\n",
        "\n",
        "logger.info(f\"Backtest simulation finished. Processed rows: {processed_rows}, Skipped rows: {skipped_rows}\")\n",
        "\n",
        "# â”€â”€â”€ Compute Additional Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Function to calculate maximum drawdown\n",
        "def calculate_max_drawdown(cumulative_profits):\n",
        "    max_drawdown = 0.0\n",
        "    peak = cumulative_profits[0]\n",
        "    for profit in cumulative_profits:\n",
        "        if profit > peak:\n",
        "            peak = profit\n",
        "        drawdown = peak - profit\n",
        "        if drawdown > max_drawdown:\n",
        "            max_drawdown = drawdown\n",
        "    return max_drawdown\n",
        "\n",
        "# Function to calculate simplified Sharpe Ratio\n",
        "def calculate_sharpe_ratio(profits, risk_free_rate=0.0):\n",
        "    if len(profits) <= 1:\n",
        "        return 0.0\n",
        "    returns = np.array(profits)\n",
        "    avg_return = np.mean(returns)\n",
        "    std_return = np.std(returns, ddof=1)\n",
        "    if std_return == 0:\n",
        "        return 0.0\n",
        "    sharpe = (avg_return - risk_free_rate) / std_return\n",
        "    return sharpe\n",
        "\n",
        "# Random Agent Metrics\n",
        "rnd_total_trades = rnd_wins + rnd_losses\n",
        "rnd_win_rate = (rnd_wins / rnd_total_trades * 100) if rnd_total_trades > 0 else 0.0\n",
        "rnd_avg_win = np.mean(rnd_win_profits) if rnd_win_profits else 0.0\n",
        "rnd_avg_loss = np.mean(rnd_loss_profits) if rnd_loss_profits else 0.0\n",
        "rnd_win_loss_ratio = rnd_wins / rnd_losses if rnd_losses > 0 else float('inf')\n",
        "rnd_max_drawdown = calculate_max_drawdown(rnd_cum_profits)\n",
        "rnd_avg_investment = np.mean([inv for inv in rnd_investments if inv > 0]) if any(inv > 0 for inv in rnd_investments) else 0.0\n",
        "rnd_sharpe = calculate_sharpe_ratio([p for p in rnd_investments if p != 0])\n",
        "\n",
        "# LGBM Agent Metrics\n",
        "mdl_total_trades = mdl_wins + mdl_losses\n",
        "mdl_win_rate = (mdl_wins / mdl_total_trades * 100) if mdl_total_trades > 0 else 0.0\n",
        "mdl_avg_win = np.mean(mdl_win_profits) if mdl_win_profits else 0.0\n",
        "mdl_avg_loss = np.mean(mdl_loss_profits) if mdl_loss_profits else 0.0\n",
        "mdl_win_loss_ratio = mdl_wins / mdl_losses if mdl_losses > 0 else float('inf')\n",
        "mdl_max_drawdown = calculate_max_drawdown(mdl_cum_profits)\n",
        "mdl_avg_investment = np.mean([inv for inv in mdl_investments if inv > 0]) if any(inv > 0 for inv in mdl_investments) else 0.0\n",
        "mdl_hit_rate_6m = (mdl_correct_6m / mdl_predictions_6m * 100) if mdl_predictions_6m > 0 else 0.0\n",
        "mdl_hit_rate_3y = (mdl_correct_3y / mdl_predictions_3y * 100) if mdl_predictions_3y > 0 else 0.0\n",
        "mdl_sharpe = calculate_sharpe_ratio([p for p in mdl_investments if p != 0])\n",
        "\n",
        "# Neural Network Agent Metrics\n",
        "nn_total_trades = nn_wins + nn_losses\n",
        "nn_win_rate = (nn_wins / nn_total_trades * 100) if nn_total_trades > 0 else 0.0\n",
        "nn_avg_win = np.mean(nn_win_profits) if nn_win_profits else 0.0\n",
        "nn_avg_loss = np.mean(nn_loss_profits) if nn_loss_profits else 0.0\n",
        "nn_win_loss_ratio = nn_wins / nn_losses if nn_losses > 0 else float('inf')\n",
        "nn_max_drawdown = calculate_max_drawdown(nn_cum_profits)\n",
        "nn_avg_investment = np.mean([inv for inv in nn_investments if inv > 0]) if any(inv > 0 for inv in nn_investments) else 0.0\n",
        "nn_hit_rate_6m = (nn_correct_6m / nn_predictions_6m * 100) if nn_predictions_6m > 0 else 0.0\n",
        "nn_hit_rate_3y = (nn_correct_3y / nn_predictions_3y * 100) if nn_predictions_3y > 0 else 0.0\n",
        "nn_sharpe = calculate_sharpe_ratio([p for p in nn_investments if p != 0])\n",
        "\n",
        "# Ensemble Agent Metrics\n",
        "ens_total_trades       = ens_wins + ens_losses\n",
        "ens_win_rate           = (ens_wins / ens_total_trades * 100) if ens_total_trades > 0 else 0.0\n",
        "ens_avg_win            = np.mean(ens_win_profits) if ens_win_profits else 0.0\n",
        "ens_avg_loss           = np.mean(ens_loss_profits) if ens_loss_profits else 0.0\n",
        "ens_win_loss_ratio     = ens_wins / ens_losses if ens_losses > 0 else float('inf')\n",
        "ens_max_drawdown       = calculate_max_drawdown(ens_cum_profits)\n",
        "ens_avg_investment     = np.mean([inv for inv in ens_investments if inv > 0]) if any(inv > 0 for inv in ens_investments) else 0.0\n",
        "ens_hit_rate_6m        = (ens_correct_6m / ens_predictions_6m * 100) if ens_predictions_6m > 0 else 0.0\n",
        "ens_hit_rate_3y        = (ens_correct_3y / ens_predictions_3y * 100) if ens_predictions_3y > 0 else 0.0\n",
        "ens_sharpe             = calculate_sharpe_ratio([p for p in ens_investments if p != 0])\n",
        "\n",
        "\n",
        "# â”€â”€â”€ 6) Save Results & Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "logger.info(\"Saving simulation results...\")\n",
        "try:\n",
        "    history_df = pd.DataFrame(history)\n",
        "    output_file = \"ipo_investment_per_ipo_100k_chrono101.csv\"\n",
        "    history_df.to_csv(output_file, index=False, float_format='%.4f')\n",
        "    logger.info(f\"Simulation results saved to {output_file}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to save results to {output_file}: {e}\")\n",
        "    logger.error(traceback.format_exc())\n",
        "    raise RuntimeError(\"Failed to save results\") from e\n",
        "\n",
        "# --- Display Summary ---\n",
        "try:\n",
        "    print(\"\\n\" + \"=\" * 10 + \" Simulation Summary \" + \"=\" * 10)\n",
        "    print(f\"Initial Capital per Strategy: ${initial_capital:,.2f}\")\n",
        "    print(f\"Investment per Signal:        ${investment_per_signal:,.2f}\")\n",
        "    print(f\"Investment for Both Signals:  ${investment_both_signals:,.2f}\")\n",
        "    print(f\"Prediction Threshold:         {prediction_threshold:.2f}\")\n",
        "    print(f\"Total Opportunities in Test Set: {len(df_test)}\")\n",
        "    print(f\"Total Processed Opportunities:   {processed_rows}\")\n",
        "    print(f\"Total Skipped Opportunities:     {skipped_rows}\")\n",
        "    print(\"-\" * 55)\n",
        "    print(f\"{'Strategy':<18} | {'Final Capital':>15} | {'Total Profit':>15}\")\n",
        "    print(\"-\" * 55)\n",
        "\n",
        "    final_rnd_cap = initial_capital + rnd_cum\n",
        "    final_mdl_cap = initial_capital + mdl_cum\n",
        "    final_nn_cap = initial_capital + nn_cum\n",
        "\n",
        "    print(f\"{'Random':<18} | ${final_rnd_cap:14,.2f} | ${rnd_cum:14,.2f}\")\n",
        "    print(f\"{'LGBM':<18} | ${final_mdl_cap:14,.2f} | ${mdl_cum:14,.2f}\")\n",
        "    print(f\"{'Neural Network':<18} | ${final_nn_cap:14,.2f} | ${nn_cum:14,.2f}\")\n",
        "    print(f\"{'Ensemble':<18} | ${initial_capital + ens_cum:14,.2f} | ${ens_cum:14,.2f}\")\n",
        "    print(\"-\" * 55)\n",
        "\n",
        "    # --- Extended Metrics Summary ---\n",
        "    print(\"\\n\" + \"=\" * 10 + \" Extended Metrics \" + \"=\" * 10)\n",
        "    print(f\"{'Metric':<25} | {'Random':>10} | {'LGBM':>10} | {'Neural Network':>15} |{'Ensemble':>15}\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    print(f\"{'Win Rate (%)':<25} | {rnd_win_rate:>10.2f} | {mdl_win_rate:>10.2f} | {nn_win_rate:>15.2f} | {ens_win_rate:>10.2f}\")\n",
        "    print(f\"{'Total Trades':<25} | {rnd_total_trades:>10} | {mdl_total_trades:>10} | {nn_total_trades:>15} | {ens_total_trades:>10}\")\n",
        "    print(f\"{'Average Win ($)':<25} | {rnd_avg_win:>10.2f} | {mdl_avg_win:>10.2f} | {nn_avg_win:>15.2f} | {ens_avg_win:>10.2f}\")\n",
        "    print(f\"{'Average Loss ($)':<25} | {rnd_avg_loss:>10.2f} | {mdl_avg_loss:>10.2f} | {nn_avg_loss:>15.2f} | {ens_avg_loss:>10.2f}\")\n",
        "    print(f\"{'Win/Loss Ratio':<25} | {rnd_win_loss_ratio:>10.2f} | {mdl_win_loss_ratio:>10.2f} | {nn_win_loss_ratio:>15.2f} | {ens_win_loss_ratio:>10.2f}\")\n",
        "    print(f\"{'Max Drawdown ($)':<25} | {rnd_max_drawdown:>10.2f} | {mdl_max_drawdown:>10.2f} | {nn_max_drawdown:>15.2f} | {ens_max_drawdown:>10.2f}\")\n",
        "    print(f\"{'Avg Investment ($)':<25} | {rnd_avg_investment:>10.2f} | {mdl_avg_investment:>10.2f} | {nn_avg_investment:>15.2f} | {ens_avg_investment:>10.2f}\")\n",
        "    print(f\"{'Sharpe Ratio':<25} | {rnd_sharpe:>10.2f} | {mdl_sharpe:>10.2f} | {nn_sharpe:>15.2f} | {ens_sharpe:>10.2f}\")\n",
        "    #print(f\"{'Hit Rate 6M (%)':<25} | {'N/A':>10} | {mdl_hit_rate_6m:>10.2f} | {nn_hit_rate_6m:>15.2f} | {ens_hit_rate_6m:>10.2f}\")\n",
        "    #print(f\"{'Hit Rate 3Y (%)':<25} | {'N/A':>10} | {mdl_hit_rate_3y:>10.2f} | {nn_hit_rate_3y:>15.2f} | {ens_hit_rate_3y:>10.2f}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error displaying summary: {e}\")\n",
        "    logger.error(traceback.format_exc())\n",
        "    raise RuntimeError(\"Failed to display summary\") from e\n",
        "\n",
        "logger.info(\"Script finished.\")"
      ],
      "metadata": {
        "id": "XBAY8HjSKG8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SHAP values\n",
        "\n",
        "import os\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shap\n",
        "from lightgbm import Booster\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "OUTPUT_DIR = '/scratch/ghtillem/Saved_LGBM_Models/Final/final'\n",
        "DATA_FILE   = '/scratch/ghtillem/Data/EODHD_Final_maxcik50_flat_wsf_incorp_m.csv'\n",
        "TARGETS     = ['flag_3y_avg', 'flag_6m_avg']\n",
        "LEAKAGE = [\n",
        "    'six_month_price', 'three_year_price', 'six_month_30d_avg', 'three_year_30d_avg',\n",
        "    'flag_6m_price', 'flag_3y_price', 'six_month_date', 'three_year_date',\n",
        "    'sp500_sma30_centered_at_start', 'sp500_above_30sma_centered', '_merge',\n",
        "    'cik', 'title', '1Y_zipma_ROC', 'All_zipba_ROC', '1Y_zipba_ROC', 'All_sic_ROC',\n",
        "    '1Y_fye_ROC', '1Y_sic_ROC', '3Y_zipma_ROC', '3Y_zipba_ROC', 'All_median_zipba',\n",
        "    'Q1_zipba_median', 'Q1_sic_mean_y', 'Q4_sic_median_y', 'Q1_zipma_median_y',\n",
        "    'Q2_sic_mean_x', 'Q2_sic_median', 'Q0_sic_mean', 'Q0_zipba_mean', 'sic',\n",
        "    'Q1_zipma_median_x', 'Q1_zippa_mean_x', 'Q1_fye_median_x', 'Q0_fye_mean',\n",
        "    'Q1_zipba_median_y', 'Q0_zipma_ROC_x', 'Q4_sic_mean_y', 'Q0_zipba_mean_y',\n",
        "    'Q0_zipba_mean_x', 'Q4_zipma_median_x', 'Q2_fye_mean_x', 'bus_zip', 'Q4_fye_median',\n",
        "    'Q2_fye_mean_y', 'Q1_fye_mean', 'Q0_zipma_median_y', 'Q1_fye_median_y', 'Q0_fye_median_y',\n",
        "    'phone', 'ticker', 'start_date'\n",
        "]  # same list you used in training\n",
        "TEST_SIZE   = 0.30\n",
        "RANDOM_SEED = 42\n",
        "SAMPLE_N    = None\n",
        "\n",
        "# â”€â”€ LOAD DATA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "df = pd.read_csv(DATA_FILE, low_memory=False)\n",
        "df.drop(columns=[c for c in LEAKAGE if c not in TARGETS], errors='ignore', inplace=True)\n",
        "\n",
        "# â”€â”€ MAIN LOOP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "for target in TARGETS:\n",
        "    print(f\"\\n===== SHAP for {target} =====\")\n",
        "    # --- reload preprocessor & metadata ---\n",
        "    proc_candidates = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
        "                             if f.startswith(f\"{target}_preprocessor_\") and f.endswith('.joblib'))\n",
        "    preprocessor: Pipeline = joblib.load(os.path.join(OUTPUT_DIR, proc_candidates[-1]))\n",
        "    meta_candidates = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
        "                             if f.startswith(f\"{target}_preprocessor_metadata_\") and f.endswith('.json'))\n",
        "    metadata = json.load(open(os.path.join(OUTPUT_DIR, meta_candidates[-1])))\n",
        "    feature_names = metadata['feature_names']\n",
        "\n",
        "    # --- reload LightGBM booster ---\n",
        "    model_txts = sorted(f for f in os.listdir(OUTPUT_DIR)\n",
        "                        if f.startswith(target) and f.endswith('.txt'))\n",
        "    booster = Booster(model_file=os.path.join(OUTPUT_DIR, model_txts[-1]))\n",
        "\n",
        "    # --- rebuild test split ---\n",
        "    y = df[target].dropna().astype(int)\n",
        "    X = df.loc[y.index].drop(columns=[target])\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_SEED\n",
        "    )\n",
        "\n",
        "    # --- optionally subsample ---\n",
        "    if SAMPLE_N is not None and SAMPLE_N < len(X_test):\n",
        "        X_test_small = X_test.sample(n=SAMPLE_N, random_state=RANDOM_SEED)\n",
        "    else:\n",
        "        X_test_small = X_test\n",
        "\n",
        "    # --- preprocess ---\n",
        "    X_pre = preprocessor.transform(X_test_small)\n",
        "\n",
        "    # --- SHAP explainer on probability scale ---\n",
        "    explainer = shap.TreeExplainer(\n",
        "        booster,\n",
        "        model_output=\"probability\",\n",
        "        data=X_pre  # speeds up init\n",
        "    )\n",
        "    raw_shap = explainer.shap_values(X_pre)\n",
        "    # binary classifier: pick the positiveâ€class matrix\n",
        "    shap_vals = raw_shap[1] if isinstance(raw_shap, list) else raw_shap\n",
        "\n",
        "    # --- sanity check ---\n",
        "    assert shap_vals.shape == X_pre.shape, f\"SHAP shape mismatch for {target}\"\n",
        "\n",
        "    # --- aggregate importances ---\n",
        "    mean_abs = np.abs(shap_vals).mean(axis=0)\n",
        "    imp_df = (pd.DataFrame({\n",
        "                  'feature': feature_names,\n",
        "                  'mean_abs_shap': mean_abs\n",
        "              })\n",
        "              .sort_values('mean_abs_shap', ascending=False))\n",
        "\n",
        "    # --- report & save ---\n",
        "    print(f\"\\nTop 20 features for {target} (n={len(X_test_small)} test rows):\")\n",
        "    print(imp_df.head(20).to_string(index=False))\n",
        "    out_csv = os.path.join(OUTPUT_DIR, f\"{target}_shap_importance_prob_fulltest.csv\")\n",
        "    imp_df.to_csv(out_csv, index=False)\n",
        "    print(f\"â†’ Saved full ranking to {out_csv}\")\n"
      ],
      "metadata": {
        "id": "EWfZDTwCJzDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0FIm5vIFKD51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MqidTziEA0-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ziAI2S0GA07m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tFkixbMvA00d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we filter the SEC Filings data by the CIK to tickers SEC data we optained (using the unique CIK file)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# âœ… File paths\n",
        "input_file = \"/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/SEC_filings_cleaned_final10.csv\"\n",
        "output_file = \"/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/ipo_filings_filtered_by_cik.csv\"\n",
        "ipo_cik_file = \"/content/drive/MyDrive/FSE IPO Data/Filtered Data/Final/cik_unique.csv\"\n",
        "\n",
        "# âœ… Load distinct CIKs\n",
        "ipo_ciks = pd.read_csv(ipo_cik_file)\n",
        "ipo_ciks['cik'] = ipo_ciks['cik'].astype(str).str.zfill(10)\n",
        "ipo_cik_set = set(ipo_ciks['cik'].unique())\n",
        "\n",
        "# âœ… Chunk processing setup\n",
        "chunksize = 500000\n",
        "first = True\n",
        "\n",
        "# âœ… Process input file in chunks\n",
        "for chunk in pd.read_csv(input_file, chunksize=chunksize):\n",
        "    # Normalize cik format\n",
        "    chunk['cik'] = chunk['cik'].apply(lambda x: str(int(x)).zfill(10))\n",
        "\n",
        "    # Filter only rows where cik is in our IPO list\n",
        "    filtered = chunk[chunk['cik'].isin(ipo_cik_set)]\n",
        "\n",
        "    # Write out filtered rows\n",
        "    filtered.to_csv(output_file, mode='a', index=False, header=first)\n",
        "    first = False\n",
        "\n",
        "    print(f\"âœ… Processed {len(chunk):,} rows; retained {len(filtered):,} matching CIKs\")"
      ],
      "metadata": {
        "id": "8ImitrdQHCbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ws95AuiEGW-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RcRh2zm1GjWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oJJDwKoJF34m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}